[2023-03-23 01:05:39] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 01:05:39] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 01:05:39] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 01:05:39] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 01:05:39] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 01:05:39] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 01:05:39] - INFO: ###  device = cuda:0
[2023-03-23 01:05:39] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 01:05:39] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 01:05:39] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 01:05:39] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 01:05:39] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 01:05:39] - INFO: ###  split_sep = _!_
[2023-03-23 01:05:39] - INFO: ###  is_sample_shuffle = True
[2023-03-23 01:05:39] - INFO: ###  batch_size = 96
[2023-03-23 01:05:39] - INFO: ###  max_sen_len = None
[2023-03-23 01:05:39] - INFO: ###  num_labels = 8191
[2023-03-23 01:05:39] - INFO: ###  epochs = 80
[2023-03-23 01:05:39] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 01:05:39] - INFO: ###  vocab_size = 21128
[2023-03-23 01:05:39] - INFO: ###  hidden_size = 768
[2023-03-23 01:05:39] - INFO: ###  num_hidden_layers = 12
[2023-03-23 01:05:39] - INFO: ###  num_attention_heads = 12
[2023-03-23 01:05:39] - INFO: ###  hidden_act = gelu
[2023-03-23 01:05:39] - INFO: ###  intermediate_size = 3072
[2023-03-23 01:05:39] - INFO: ###  pad_token_id = 0
[2023-03-23 01:05:39] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 01:05:39] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 01:05:39] - INFO: ###  max_position_embeddings = 512
[2023-03-23 01:05:39] - INFO: ###  type_vocab_size = 2
[2023-03-23 01:05:39] - INFO: ###  initializer_range = 0.02
[2023-03-23 01:05:39] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 01:05:39] - INFO: ###  directionality = bidi
[2023-03-23 01:05:39] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 01:05:39] - INFO: ###  model_type = bert
[2023-03-23 01:05:39] - INFO: ###  pooler_fc_size = 768
[2023-03-23 01:05:39] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 01:05:39] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 01:05:39] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 01:05:39] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 01:05:41] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 01:05:42] - INFO: ## 成功载入已有模型，进行追加训练......
[2023-03-23 01:05:44] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/test_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:05:44] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/train_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:05:44] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/val_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:08:23] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 01:08:23] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 01:08:23] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 01:08:23] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 01:08:23] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 01:08:23] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 01:08:23] - INFO: ###  device = cuda:0
[2023-03-23 01:08:23] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 01:08:23] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 01:08:23] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 01:08:23] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 01:08:23] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 01:08:23] - INFO: ###  split_sep = _!_
[2023-03-23 01:08:23] - INFO: ###  is_sample_shuffle = True
[2023-03-23 01:08:23] - INFO: ###  batch_size = 64
[2023-03-23 01:08:23] - INFO: ###  max_sen_len = None
[2023-03-23 01:08:23] - INFO: ###  num_labels = 8191
[2023-03-23 01:08:23] - INFO: ###  epochs = 80
[2023-03-23 01:08:23] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 01:08:23] - INFO: ###  vocab_size = 21128
[2023-03-23 01:08:23] - INFO: ###  hidden_size = 768
[2023-03-23 01:08:23] - INFO: ###  num_hidden_layers = 12
[2023-03-23 01:08:23] - INFO: ###  num_attention_heads = 12
[2023-03-23 01:08:23] - INFO: ###  hidden_act = gelu
[2023-03-23 01:08:23] - INFO: ###  intermediate_size = 3072
[2023-03-23 01:08:23] - INFO: ###  pad_token_id = 0
[2023-03-23 01:08:23] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 01:08:23] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 01:08:23] - INFO: ###  max_position_embeddings = 512
[2023-03-23 01:08:23] - INFO: ###  type_vocab_size = 2
[2023-03-23 01:08:23] - INFO: ###  initializer_range = 0.02
[2023-03-23 01:08:23] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 01:08:23] - INFO: ###  directionality = bidi
[2023-03-23 01:08:23] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 01:08:23] - INFO: ###  model_type = bert
[2023-03-23 01:08:23] - INFO: ###  pooler_fc_size = 768
[2023-03-23 01:08:23] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 01:08:23] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 01:08:23] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 01:08:23] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 01:08:25] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 01:08:26] - INFO: ## 成功载入已有模型，进行追加训练......
[2023-03-23 01:08:27] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/test_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:08:27] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/train_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:08:27] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/val_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:08:59] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 01:08:59] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 01:08:59] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 01:08:59] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 01:08:59] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 01:08:59] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 01:08:59] - INFO: ###  device = cuda:0
[2023-03-23 01:08:59] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 01:08:59] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 01:08:59] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 01:08:59] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 01:08:59] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 01:08:59] - INFO: ###  split_sep = _!_
[2023-03-23 01:08:59] - INFO: ###  is_sample_shuffle = True
[2023-03-23 01:08:59] - INFO: ###  batch_size = 32
[2023-03-23 01:08:59] - INFO: ###  max_sen_len = None
[2023-03-23 01:08:59] - INFO: ###  num_labels = 8191
[2023-03-23 01:08:59] - INFO: ###  epochs = 80
[2023-03-23 01:08:59] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 01:08:59] - INFO: ###  vocab_size = 21128
[2023-03-23 01:08:59] - INFO: ###  hidden_size = 768
[2023-03-23 01:08:59] - INFO: ###  num_hidden_layers = 12
[2023-03-23 01:08:59] - INFO: ###  num_attention_heads = 12
[2023-03-23 01:08:59] - INFO: ###  hidden_act = gelu
[2023-03-23 01:08:59] - INFO: ###  intermediate_size = 3072
[2023-03-23 01:08:59] - INFO: ###  pad_token_id = 0
[2023-03-23 01:08:59] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 01:08:59] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 01:08:59] - INFO: ###  max_position_embeddings = 512
[2023-03-23 01:08:59] - INFO: ###  type_vocab_size = 2
[2023-03-23 01:08:59] - INFO: ###  initializer_range = 0.02
[2023-03-23 01:08:59] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 01:08:59] - INFO: ###  directionality = bidi
[2023-03-23 01:08:59] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 01:08:59] - INFO: ###  model_type = bert
[2023-03-23 01:08:59] - INFO: ###  pooler_fc_size = 768
[2023-03-23 01:08:59] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 01:08:59] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 01:08:59] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 01:08:59] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 01:09:01] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 01:09:02] - INFO: ## 成功载入已有模型，进行追加训练......
[2023-03-23 01:09:04] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/test_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:09:04] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/train_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:09:04] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/val_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:09:05] - INFO: Epoch: 0, Batch[0/22], Train loss :1.932, Train acc: 0.656
[2023-03-23 01:09:14] - INFO: Epoch: 0, Batch[10/22], Train loss :2.475, Train acc: 0.500
[2023-03-23 01:09:23] - INFO: Epoch: 0, Batch[20/22], Train loss :1.656, Train acc: 0.719
[2023-03-23 01:09:24] - INFO: Epoch: 0, Train loss: 1.738, Epoch time = 20.028s
[2023-03-23 01:09:25] - INFO: Epoch: 1, Batch[0/22], Train loss :2.001, Train acc: 0.625
[2023-03-23 01:09:33] - INFO: Epoch: 1, Batch[10/22], Train loss :1.001, Train acc: 0.812
[2023-03-23 01:09:42] - INFO: Epoch: 1, Batch[20/22], Train loss :0.864, Train acc: 0.844
[2023-03-23 01:09:43] - INFO: Epoch: 1, Train loss: 1.268, Epoch time = 19.196s
[2023-03-23 01:09:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:09:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:09:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:09:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:09:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:09:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:09:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:09:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:09:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:09:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:09:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:09:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:09:44] - INFO: Accuracy on val 0.605
[2023-03-23 01:09:46] - INFO: Epoch: 2, Batch[0/22], Train loss :1.638, Train acc: 0.688
[2023-03-23 01:09:55] - INFO: Epoch: 2, Batch[10/22], Train loss :1.016, Train acc: 0.812
[2023-03-23 01:10:04] - INFO: Epoch: 2, Batch[20/22], Train loss :0.972, Train acc: 0.781
[2023-03-23 01:10:04] - INFO: Epoch: 2, Train loss: 1.019, Epoch time = 19.309s
[2023-03-23 01:10:05] - INFO: Epoch: 3, Batch[0/22], Train loss :0.666, Train acc: 0.844
[2023-03-23 01:10:14] - INFO: Epoch: 3, Batch[10/22], Train loss :0.870, Train acc: 0.812
[2023-03-23 01:10:23] - INFO: Epoch: 3, Batch[20/22], Train loss :1.487, Train acc: 0.656
[2023-03-23 01:10:24] - INFO: Epoch: 3, Train loss: 0.889, Epoch time = 19.358s
[2023-03-23 01:10:24] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:10:24] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:10:24] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:10:24] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:10:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:10:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:10:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:10:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:10:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:10:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:10:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:10:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:10:25] - INFO: Accuracy on val 0.512
[2023-03-23 01:10:26] - INFO: Epoch: 4, Batch[0/22], Train loss :0.956, Train acc: 0.750
[2023-03-23 01:10:35] - INFO: Epoch: 4, Batch[10/22], Train loss :0.664, Train acc: 0.812
[2023-03-23 01:10:44] - INFO: Epoch: 4, Batch[20/22], Train loss :1.113, Train acc: 0.719
[2023-03-23 01:10:45] - INFO: Epoch: 4, Train loss: 0.845, Epoch time = 19.366s
[2023-03-23 01:10:46] - INFO: Epoch: 5, Batch[0/22], Train loss :0.732, Train acc: 0.844
[2023-03-23 01:10:55] - INFO: Epoch: 5, Batch[10/22], Train loss :1.038, Train acc: 0.781
[2023-03-23 01:11:04] - INFO: Epoch: 5, Batch[20/22], Train loss :0.217, Train acc: 0.969
[2023-03-23 01:11:04] - INFO: Epoch: 5, Train loss: 0.700, Epoch time = 19.373s
[2023-03-23 01:11:04] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:11:04] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:11:05] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:11:05] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:11:05] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:11:05] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:11:05] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:11:05] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:11:06] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:11:06] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:11:06] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:11:06] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:11:06] - INFO: Accuracy on val 0.622
[2023-03-23 01:11:07] - INFO: Epoch: 6, Batch[0/22], Train loss :0.776, Train acc: 0.844
[2023-03-23 01:11:16] - INFO: Epoch: 6, Batch[10/22], Train loss :0.686, Train acc: 0.844
[2023-03-23 01:11:25] - INFO: Epoch: 6, Batch[20/22], Train loss :0.545, Train acc: 0.875
[2023-03-23 01:11:26] - INFO: Epoch: 6, Train loss: 0.557, Epoch time = 19.414s
[2023-03-23 01:11:27] - INFO: Epoch: 7, Batch[0/22], Train loss :0.307, Train acc: 0.906
[2023-03-23 01:11:36] - INFO: Epoch: 7, Batch[10/22], Train loss :0.296, Train acc: 0.969
[2023-03-23 01:11:45] - INFO: Epoch: 7, Batch[20/22], Train loss :0.463, Train acc: 0.906
[2023-03-23 01:11:45] - INFO: Epoch: 7, Train loss: 0.478, Epoch time = 19.439s
[2023-03-23 01:11:45] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:11:45] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:11:46] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:11:46] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:11:46] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:11:46] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:11:46] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:11:46] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:11:47] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:11:47] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:11:47] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:11:47] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:11:47] - INFO: Accuracy on val 0.686
[2023-03-23 01:11:48] - INFO: Epoch: 8, Batch[0/22], Train loss :0.317, Train acc: 0.938
[2023-03-23 01:11:57] - INFO: Epoch: 8, Batch[10/22], Train loss :0.427, Train acc: 0.906
[2023-03-23 01:12:06] - INFO: Epoch: 8, Batch[20/22], Train loss :0.643, Train acc: 0.875
[2023-03-23 01:12:07] - INFO: Epoch: 8, Train loss: 0.393, Epoch time = 19.449s
[2023-03-23 01:12:08] - INFO: Epoch: 9, Batch[0/22], Train loss :0.523, Train acc: 0.906
[2023-03-23 01:12:17] - INFO: Epoch: 9, Batch[10/22], Train loss :0.565, Train acc: 0.875
[2023-03-23 01:12:26] - INFO: Epoch: 9, Batch[20/22], Train loss :0.298, Train acc: 0.938
[2023-03-23 01:12:26] - INFO: Epoch: 9, Train loss: 0.362, Epoch time = 19.455s
[2023-03-23 01:12:27] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:12:27] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:12:27] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:12:27] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:12:27] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:12:27] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:12:27] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:12:27] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:12:28] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:12:28] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:12:28] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:12:28] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:12:28] - INFO: Accuracy on val 0.610
[2023-03-23 01:12:29] - INFO: Epoch: 10, Batch[0/22], Train loss :0.521, Train acc: 0.844
[2023-03-23 01:12:38] - INFO: Epoch: 10, Batch[10/22], Train loss :0.317, Train acc: 0.938
[2023-03-23 01:12:47] - INFO: Epoch: 10, Batch[20/22], Train loss :0.209, Train acc: 0.938
[2023-03-23 01:12:47] - INFO: Epoch: 10, Train loss: 0.357, Epoch time = 19.490s
[2023-03-23 01:12:48] - INFO: Epoch: 11, Batch[0/22], Train loss :0.308, Train acc: 0.938
[2023-03-23 01:12:57] - INFO: Epoch: 11, Batch[10/22], Train loss :0.226, Train acc: 0.969
[2023-03-23 01:13:06] - INFO: Epoch: 11, Batch[20/22], Train loss :0.338, Train acc: 0.875
[2023-03-23 01:13:07] - INFO: Epoch: 11, Train loss: 0.387, Epoch time = 19.484s
[2023-03-23 01:13:07] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:13:07] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:13:08] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:13:08] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:13:08] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:13:08] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:13:08] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:13:08] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:13:08] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:13:08] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:13:09] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:13:09] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:13:09] - INFO: Accuracy on val 0.663
[2023-03-23 01:13:09] - INFO: Epoch: 12, Batch[0/22], Train loss :0.363, Train acc: 0.938
[2023-03-23 01:13:19] - INFO: Epoch: 12, Batch[10/22], Train loss :0.241, Train acc: 0.938
[2023-03-23 01:13:28] - INFO: Epoch: 12, Batch[20/22], Train loss :0.468, Train acc: 0.875
[2023-03-23 01:13:28] - INFO: Epoch: 12, Train loss: 0.329, Epoch time = 19.467s
[2023-03-23 01:13:29] - INFO: Epoch: 13, Batch[0/22], Train loss :0.213, Train acc: 1.000
[2023-03-23 01:13:38] - INFO: Epoch: 13, Batch[10/22], Train loss :0.210, Train acc: 0.938
[2023-03-23 01:13:47] - INFO: Epoch: 13, Batch[20/22], Train loss :0.268, Train acc: 0.938
[2023-03-23 01:13:47] - INFO: Epoch: 13, Train loss: 0.283, Epoch time = 19.338s
[2023-03-23 01:13:48] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:13:48] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:13:48] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:13:48] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:13:48] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:13:48] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:13:49] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:13:49] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:13:49] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:13:49] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:13:49] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:13:49] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:13:49] - INFO: Accuracy on val 0.663
[2023-03-23 01:13:50] - INFO: Epoch: 14, Batch[0/22], Train loss :0.049, Train acc: 1.000
[2023-03-23 01:13:59] - INFO: Epoch: 14, Batch[10/22], Train loss :0.180, Train acc: 0.969
[2023-03-23 01:14:08] - INFO: Epoch: 14, Batch[20/22], Train loss :0.031, Train acc: 1.000
[2023-03-23 01:14:08] - INFO: Epoch: 14, Train loss: 0.211, Epoch time = 19.477s
[2023-03-23 01:14:09] - INFO: Epoch: 15, Batch[0/22], Train loss :0.029, Train acc: 1.000
[2023-03-23 01:14:18] - INFO: Epoch: 15, Batch[10/22], Train loss :0.129, Train acc: 1.000
[2023-03-23 01:14:27] - INFO: Epoch: 15, Batch[20/22], Train loss :0.040, Train acc: 1.000
[2023-03-23 01:14:28] - INFO: Epoch: 15, Train loss: 0.182, Epoch time = 19.453s
[2023-03-23 01:14:28] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:14:28] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:14:29] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:14:29] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:14:29] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:14:29] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:14:29] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:14:29] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:14:29] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:14:29] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:14:30] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:14:30] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:14:30] - INFO: Accuracy on val 0.616
[2023-03-23 01:14:31] - INFO: Epoch: 16, Batch[0/22], Train loss :0.023, Train acc: 1.000
[2023-03-23 01:14:40] - INFO: Epoch: 16, Batch[10/22], Train loss :0.216, Train acc: 0.938
[2023-03-23 01:14:49] - INFO: Epoch: 16, Batch[20/22], Train loss :0.127, Train acc: 0.969
[2023-03-23 01:14:49] - INFO: Epoch: 16, Train loss: 0.140, Epoch time = 19.444s
[2023-03-23 01:14:50] - INFO: Epoch: 17, Batch[0/22], Train loss :0.150, Train acc: 0.969
[2023-03-23 01:14:59] - INFO: Epoch: 17, Batch[10/22], Train loss :0.078, Train acc: 1.000
[2023-03-23 01:15:08] - INFO: Epoch: 17, Batch[20/22], Train loss :0.095, Train acc: 1.000
[2023-03-23 01:15:08] - INFO: Epoch: 17, Train loss: 0.126, Epoch time = 19.413s
[2023-03-23 01:15:09] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:15:09] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:15:09] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:15:09] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:15:09] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:15:09] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:15:10] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:15:10] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:15:10] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:15:10] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:15:10] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:15:10] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:15:10] - INFO: Accuracy on val 0.616
[2023-03-23 01:15:11] - INFO: Epoch: 18, Batch[0/22], Train loss :0.253, Train acc: 0.938
[2023-03-23 01:15:20] - INFO: Epoch: 18, Batch[10/22], Train loss :0.309, Train acc: 0.969
[2023-03-23 01:15:29] - INFO: Epoch: 18, Batch[20/22], Train loss :0.055, Train acc: 1.000
[2023-03-23 01:15:30] - INFO: Epoch: 18, Train loss: 0.110, Epoch time = 19.453s
[2023-03-23 01:15:30] - INFO: Epoch: 19, Batch[0/22], Train loss :0.053, Train acc: 1.000
[2023-03-23 01:16:01] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 01:16:01] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 01:16:01] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 01:16:01] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 01:16:01] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 01:16:01] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 01:16:01] - INFO: ###  device = cuda:0
[2023-03-23 01:16:01] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 01:16:01] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 01:16:01] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 01:16:01] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 01:16:01] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 01:16:01] - INFO: ###  split_sep = _!_
[2023-03-23 01:16:01] - INFO: ###  is_sample_shuffle = True
[2023-03-23 01:16:01] - INFO: ###  batch_size = 48
[2023-03-23 01:16:01] - INFO: ###  max_sen_len = None
[2023-03-23 01:16:01] - INFO: ###  num_labels = 8191
[2023-03-23 01:16:01] - INFO: ###  epochs = 15
[2023-03-23 01:16:01] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 01:16:01] - INFO: ###  vocab_size = 21128
[2023-03-23 01:16:01] - INFO: ###  hidden_size = 768
[2023-03-23 01:16:01] - INFO: ###  num_hidden_layers = 12
[2023-03-23 01:16:01] - INFO: ###  num_attention_heads = 12
[2023-03-23 01:16:01] - INFO: ###  hidden_act = gelu
[2023-03-23 01:16:01] - INFO: ###  intermediate_size = 3072
[2023-03-23 01:16:01] - INFO: ###  pad_token_id = 0
[2023-03-23 01:16:01] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 01:16:01] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 01:16:01] - INFO: ###  max_position_embeddings = 512
[2023-03-23 01:16:01] - INFO: ###  type_vocab_size = 2
[2023-03-23 01:16:01] - INFO: ###  initializer_range = 0.02
[2023-03-23 01:16:01] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 01:16:01] - INFO: ###  directionality = bidi
[2023-03-23 01:16:01] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 01:16:01] - INFO: ###  model_type = bert
[2023-03-23 01:16:01] - INFO: ###  pooler_fc_size = 768
[2023-03-23 01:16:01] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 01:16:01] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 01:16:01] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 01:16:01] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 01:16:04] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 01:16:06] - INFO: ## 成功载入已有模型，进行追加训练......
[2023-03-23 01:16:06] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/test_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:16:06] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/train_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:16:06] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/val_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:16:58] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 01:16:58] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 01:16:58] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 01:16:58] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 01:16:58] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 01:16:58] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 01:16:58] - INFO: ###  device = cuda:0
[2023-03-23 01:16:58] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 01:16:58] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 01:16:58] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 01:16:58] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 01:16:58] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 01:16:58] - INFO: ###  split_sep = _!_
[2023-03-23 01:16:58] - INFO: ###  is_sample_shuffle = True
[2023-03-23 01:16:58] - INFO: ###  batch_size = 16
[2023-03-23 01:16:58] - INFO: ###  max_sen_len = None
[2023-03-23 01:16:58] - INFO: ###  num_labels = 8191
[2023-03-23 01:16:58] - INFO: ###  epochs = 15
[2023-03-23 01:16:58] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 01:16:58] - INFO: ###  vocab_size = 21128
[2023-03-23 01:16:58] - INFO: ###  hidden_size = 768
[2023-03-23 01:16:58] - INFO: ###  num_hidden_layers = 12
[2023-03-23 01:16:58] - INFO: ###  num_attention_heads = 12
[2023-03-23 01:16:58] - INFO: ###  hidden_act = gelu
[2023-03-23 01:16:58] - INFO: ###  intermediate_size = 3072
[2023-03-23 01:16:58] - INFO: ###  pad_token_id = 0
[2023-03-23 01:16:58] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 01:16:58] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 01:16:58] - INFO: ###  max_position_embeddings = 512
[2023-03-23 01:16:58] - INFO: ###  type_vocab_size = 2
[2023-03-23 01:16:58] - INFO: ###  initializer_range = 0.02
[2023-03-23 01:16:58] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 01:16:58] - INFO: ###  directionality = bidi
[2023-03-23 01:16:58] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 01:16:58] - INFO: ###  model_type = bert
[2023-03-23 01:16:58] - INFO: ###  pooler_fc_size = 768
[2023-03-23 01:16:58] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 01:16:58] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 01:16:58] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 01:16:58] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 01:17:00] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 01:17:02] - INFO: ## 成功载入已有模型，进行追加训练......
[2023-03-23 01:17:02] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/test_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:17:02] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/train_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:17:02] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/val_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:17:04] - INFO: Epoch: 0, Batch[0/43], Train loss :0.413, Train acc: 0.938
[2023-03-23 01:17:08] - INFO: Epoch: 0, Batch[10/43], Train loss :0.166, Train acc: 1.000
[2023-03-23 01:17:13] - INFO: Epoch: 0, Batch[20/43], Train loss :0.051, Train acc: 1.000
[2023-03-23 01:17:17] - INFO: Epoch: 0, Batch[30/43], Train loss :0.174, Train acc: 1.000
[2023-03-23 01:17:22] - INFO: Epoch: 0, Batch[40/43], Train loss :0.928, Train acc: 0.812
[2023-03-23 01:17:23] - INFO: Epoch: 0, Train loss: 0.375, Epoch time = 20.358s
[2023-03-23 01:17:23] - INFO: Epoch: 1, Batch[0/43], Train loss :0.324, Train acc: 0.938
[2023-03-23 01:17:28] - INFO: Epoch: 1, Batch[10/43], Train loss :0.499, Train acc: 0.875
[2023-03-23 01:17:32] - INFO: Epoch: 1, Batch[20/43], Train loss :0.204, Train acc: 1.000
[2023-03-23 01:17:37] - INFO: Epoch: 1, Batch[30/43], Train loss :0.528, Train acc: 0.875
[2023-03-23 01:17:42] - INFO: Epoch: 1, Batch[40/43], Train loss :0.441, Train acc: 0.938
[2023-03-23 01:17:42] - INFO: Epoch: 1, Train loss: 0.341, Epoch time = 19.690s
[2023-03-23 01:17:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:17:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:17:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:17:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:17:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:17:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:17:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:17:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:17:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:17:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:17:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:17:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:17:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:17:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:17:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:17:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:17:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:17:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:17:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:17:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:17:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:17:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:17:44] - INFO: Accuracy on val 0.680
[2023-03-23 01:17:45] - INFO: Epoch: 2, Batch[0/43], Train loss :0.294, Train acc: 0.938
[2023-03-23 01:17:50] - INFO: Epoch: 2, Batch[10/43], Train loss :0.206, Train acc: 0.938
[2023-03-23 01:17:54] - INFO: Epoch: 2, Batch[20/43], Train loss :0.062, Train acc: 1.000
[2023-03-23 01:17:59] - INFO: Epoch: 2, Batch[30/43], Train loss :0.027, Train acc: 1.000
[2023-03-23 01:18:04] - INFO: Epoch: 2, Batch[40/43], Train loss :0.984, Train acc: 0.812
[2023-03-23 01:18:04] - INFO: Epoch: 2, Train loss: 0.315, Epoch time = 19.769s
[2023-03-23 01:18:05] - INFO: Epoch: 3, Batch[0/43], Train loss :0.134, Train acc: 1.000
[2023-03-23 01:18:09] - INFO: Epoch: 3, Batch[10/43], Train loss :0.055, Train acc: 1.000
[2023-03-23 01:18:14] - INFO: Epoch: 3, Batch[20/43], Train loss :0.117, Train acc: 1.000
[2023-03-23 01:18:19] - INFO: Epoch: 3, Batch[30/43], Train loss :0.587, Train acc: 0.875
[2023-03-23 01:18:23] - INFO: Epoch: 3, Batch[40/43], Train loss :0.198, Train acc: 1.000
[2023-03-23 01:18:24] - INFO: Epoch: 3, Train loss: 0.300, Epoch time = 19.806s
[2023-03-23 01:18:24] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:18:24] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:18:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:18:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:18:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:18:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:18:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:18:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:18:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:18:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:18:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:18:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:18:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:18:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:18:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:18:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:18:26] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:18:26] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:18:26] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:18:26] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:18:26] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:18:26] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:18:26] - INFO: Accuracy on val 0.669
[2023-03-23 01:18:26] - INFO: Epoch: 4, Batch[0/43], Train loss :0.100, Train acc: 1.000
[2023-03-23 01:18:31] - INFO: Epoch: 4, Batch[10/43], Train loss :0.218, Train acc: 1.000
[2023-03-23 01:18:36] - INFO: Epoch: 4, Batch[20/43], Train loss :0.013, Train acc: 1.000
[2023-03-23 01:18:40] - INFO: Epoch: 4, Batch[30/43], Train loss :0.570, Train acc: 0.938
[2023-03-23 01:18:45] - INFO: Epoch: 4, Batch[40/43], Train loss :0.762, Train acc: 0.812
[2023-03-23 01:18:46] - INFO: Epoch: 4, Train loss: 0.286, Epoch time = 19.811s
[2023-03-23 01:18:46] - INFO: Epoch: 5, Batch[0/43], Train loss :0.116, Train acc: 1.000
[2023-03-23 01:18:51] - INFO: Epoch: 5, Batch[10/43], Train loss :0.187, Train acc: 1.000
[2023-03-23 01:18:55] - INFO: Epoch: 5, Batch[20/43], Train loss :0.274, Train acc: 0.938
[2023-03-23 01:19:00] - INFO: Epoch: 5, Batch[30/43], Train loss :0.565, Train acc: 0.875
[2023-03-23 01:19:05] - INFO: Epoch: 5, Batch[40/43], Train loss :0.271, Train acc: 0.938
[2023-03-23 01:19:06] - INFO: Epoch: 5, Train loss: 0.275, Epoch time = 19.872s
[2023-03-23 01:19:06] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:19:06] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:19:06] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:19:06] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:19:06] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:19:06] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:19:06] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:19:06] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:19:06] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:19:06] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:19:07] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:19:07] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:19:07] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:19:07] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:19:07] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:19:07] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:19:07] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:19:07] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:19:07] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:19:07] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:19:07] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:19:07] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:19:07] - INFO: Accuracy on val 0.645
[2023-03-23 01:19:08] - INFO: Epoch: 6, Batch[0/43], Train loss :0.089, Train acc: 1.000
[2023-03-23 01:19:12] - INFO: Epoch: 6, Batch[10/43], Train loss :0.230, Train acc: 0.938
[2023-03-23 01:19:17] - INFO: Epoch: 6, Batch[20/43], Train loss :0.300, Train acc: 0.938
[2023-03-23 01:21:38] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 01:21:38] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 01:21:38] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 01:21:38] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 01:21:38] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 01:21:38] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 01:21:38] - INFO: ###  device = cuda:0
[2023-03-23 01:21:38] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 01:21:38] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 01:21:38] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 01:21:38] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 01:21:38] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 01:21:38] - INFO: ###  split_sep = _!_
[2023-03-23 01:21:38] - INFO: ###  is_sample_shuffle = True
[2023-03-23 01:21:38] - INFO: ###  batch_size = 32
[2023-03-23 01:21:38] - INFO: ###  max_sen_len = None
[2023-03-23 01:21:38] - INFO: ###  num_labels = 8191
[2023-03-23 01:21:38] - INFO: ###  epochs = 25
[2023-03-23 01:21:38] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 01:21:38] - INFO: ###  vocab_size = 21128
[2023-03-23 01:21:38] - INFO: ###  hidden_size = 768
[2023-03-23 01:21:38] - INFO: ###  num_hidden_layers = 12
[2023-03-23 01:21:38] - INFO: ###  num_attention_heads = 12
[2023-03-23 01:21:38] - INFO: ###  hidden_act = gelu
[2023-03-23 01:21:38] - INFO: ###  intermediate_size = 3072
[2023-03-23 01:21:38] - INFO: ###  pad_token_id = 0
[2023-03-23 01:21:38] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 01:21:38] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 01:21:38] - INFO: ###  max_position_embeddings = 512
[2023-03-23 01:21:38] - INFO: ###  type_vocab_size = 2
[2023-03-23 01:21:38] - INFO: ###  initializer_range = 0.02
[2023-03-23 01:21:38] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 01:21:38] - INFO: ###  directionality = bidi
[2023-03-23 01:21:38] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 01:21:38] - INFO: ###  model_type = bert
[2023-03-23 01:21:38] - INFO: ###  pooler_fc_size = 768
[2023-03-23 01:21:38] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 01:21:38] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 01:21:38] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 01:21:38] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 01:21:40] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 01:21:41] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/test_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:21:41] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/train_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:21:41] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/val_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:21:43] - INFO: Epoch: 0, Batch[0/22], Train loss :9.504, Train acc: 0.000
[2023-03-23 01:21:52] - INFO: Epoch: 0, Batch[10/22], Train loss :4.185, Train acc: 0.469
[2023-03-23 01:22:01] - INFO: Epoch: 0, Batch[20/22], Train loss :2.747, Train acc: 0.562
[2023-03-23 01:22:01] - INFO: Epoch: 0, Train loss: 3.878, Epoch time = 19.966s
[2023-03-23 01:22:02] - INFO: Epoch: 1, Batch[0/22], Train loss :2.414, Train acc: 0.594
[2023-03-23 01:22:11] - INFO: Epoch: 1, Batch[10/22], Train loss :1.455, Train acc: 0.688
[2023-03-23 01:22:20] - INFO: Epoch: 1, Batch[20/22], Train loss :1.627, Train acc: 0.656
[2023-03-23 01:22:21] - INFO: Epoch: 1, Train loss: 1.908, Epoch time = 19.195s
[2023-03-23 01:22:21] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:22:21] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:22:21] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:22:21] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:22:22] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:22:22] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:22:22] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:22:22] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:22:22] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:22:22] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:22:22] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:22:22] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:22:22] - INFO: Accuracy on val 0.640
[2023-03-23 01:22:24] - INFO: Epoch: 2, Batch[0/22], Train loss :1.602, Train acc: 0.719
[2023-03-23 01:22:33] - INFO: Epoch: 2, Batch[10/22], Train loss :1.552, Train acc: 0.656
[2023-03-23 01:22:41] - INFO: Epoch: 2, Batch[20/22], Train loss :2.303, Train acc: 0.594
[2023-03-23 01:22:42] - INFO: Epoch: 2, Train loss: 1.847, Epoch time = 19.265s
[2023-03-23 01:22:43] - INFO: Epoch: 3, Batch[0/22], Train loss :2.162, Train acc: 0.531
[2023-03-23 01:22:52] - INFO: Epoch: 3, Batch[10/22], Train loss :1.245, Train acc: 0.750
[2023-03-23 01:23:01] - INFO: Epoch: 3, Batch[20/22], Train loss :2.523, Train acc: 0.375
[2023-03-23 01:23:01] - INFO: Epoch: 3, Train loss: 1.856, Epoch time = 19.352s
[2023-03-23 01:23:02] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:23:02] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:23:02] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:23:02] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:23:02] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:23:02] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:23:03] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:23:03] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:23:03] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:23:03] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:23:03] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:23:03] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:23:03] - INFO: Accuracy on val 0.640
[2023-03-23 01:23:04] - INFO: Epoch: 4, Batch[0/22], Train loss :1.617, Train acc: 0.656
[2023-03-23 01:23:13] - INFO: Epoch: 4, Batch[10/22], Train loss :2.165, Train acc: 0.531
[2023-03-23 01:23:22] - INFO: Epoch: 4, Batch[20/22], Train loss :1.653, Train acc: 0.594
[2023-03-23 01:23:22] - INFO: Epoch: 4, Train loss: 1.824, Epoch time = 19.418s
[2023-03-23 01:23:23] - INFO: Epoch: 5, Batch[0/22], Train loss :2.019, Train acc: 0.594
[2023-03-23 01:23:32] - INFO: Epoch: 5, Batch[10/22], Train loss :1.977, Train acc: 0.562
[2023-03-23 01:23:41] - INFO: Epoch: 5, Batch[20/22], Train loss :1.154, Train acc: 0.812
[2023-03-23 01:23:42] - INFO: Epoch: 5, Train loss: 1.839, Epoch time = 19.385s
[2023-03-23 01:23:42] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:23:42] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:23:42] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:23:42] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:23:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:23:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:23:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:23:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:23:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:23:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:23:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:23:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:23:43] - INFO: Accuracy on val 0.640
[2023-03-23 01:23:44] - INFO: Epoch: 6, Batch[0/22], Train loss :1.396, Train acc: 0.719
[2023-03-23 01:23:53] - INFO: Epoch: 6, Batch[10/22], Train loss :2.341, Train acc: 0.500
[2023-03-23 01:24:02] - INFO: Epoch: 6, Batch[20/22], Train loss :1.314, Train acc: 0.781
[2023-03-23 01:24:03] - INFO: Epoch: 6, Train loss: 1.826, Epoch time = 19.353s
[2023-03-23 01:24:04] - INFO: Epoch: 7, Batch[0/22], Train loss :1.320, Train acc: 0.750
[2023-03-23 01:24:13] - INFO: Epoch: 7, Batch[10/22], Train loss :2.117, Train acc: 0.500
[2023-03-23 01:24:22] - INFO: Epoch: 7, Batch[20/22], Train loss :1.984, Train acc: 0.562
[2023-03-23 01:24:22] - INFO: Epoch: 7, Train loss: 1.824, Epoch time = 19.360s
[2023-03-23 01:24:22] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:24:22] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:24:23] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:24:23] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:24:23] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:24:23] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:24:23] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:24:23] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:24:24] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:24:24] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:24:24] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:24:24] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:24:24] - INFO: Accuracy on val 0.640
[2023-03-23 01:24:25] - INFO: Epoch: 8, Batch[0/22], Train loss :1.712, Train acc: 0.594
[2023-03-23 01:24:34] - INFO: Epoch: 8, Batch[10/22], Train loss :1.879, Train acc: 0.562
[2023-03-23 01:24:43] - INFO: Epoch: 8, Batch[20/22], Train loss :1.554, Train acc: 0.688
[2023-03-23 01:24:43] - INFO: Epoch: 8, Train loss: 1.862, Epoch time = 19.362s
[2023-03-23 01:24:44] - INFO: Epoch: 9, Batch[0/22], Train loss :1.449, Train acc: 0.656
[2023-03-23 01:24:53] - INFO: Epoch: 9, Batch[10/22], Train loss :1.982, Train acc: 0.531
[2023-03-23 01:25:02] - INFO: Epoch: 9, Batch[20/22], Train loss :2.278, Train acc: 0.500
[2023-03-23 01:25:02] - INFO: Epoch: 9, Train loss: 1.818, Epoch time = 19.364s
[2023-03-23 01:25:03] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:25:03] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:25:03] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:25:03] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:25:03] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:25:03] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:25:04] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:25:04] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:25:04] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:25:04] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:25:04] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:25:04] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:25:04] - INFO: Accuracy on val 0.640
[2023-03-23 01:25:05] - INFO: Epoch: 10, Batch[0/22], Train loss :1.723, Train acc: 0.656
[2023-03-23 01:25:14] - INFO: Epoch: 10, Batch[10/22], Train loss :1.378, Train acc: 0.719
[2023-03-23 01:25:23] - INFO: Epoch: 10, Batch[20/22], Train loss :1.944, Train acc: 0.594
[2023-03-23 01:25:23] - INFO: Epoch: 10, Train loss: 1.794, Epoch time = 19.360s
[2023-03-23 01:25:24] - INFO: Epoch: 11, Batch[0/22], Train loss :2.147, Train acc: 0.500
[2023-03-23 01:25:33] - INFO: Epoch: 11, Batch[10/22], Train loss :1.674, Train acc: 0.656
[2023-03-23 01:25:42] - INFO: Epoch: 11, Batch[20/22], Train loss :1.533, Train acc: 0.719
[2023-03-23 01:25:43] - INFO: Epoch: 11, Train loss: 1.805, Epoch time = 19.360s
[2023-03-23 01:25:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:25:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:25:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:25:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:25:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:25:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:25:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:25:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:25:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:25:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:25:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:25:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:25:44] - INFO: Accuracy on val 0.640
[2023-03-23 01:25:45] - INFO: Epoch: 12, Batch[0/22], Train loss :1.698, Train acc: 0.625
[2023-03-23 01:25:54] - INFO: Epoch: 12, Batch[10/22], Train loss :1.442, Train acc: 0.719
[2023-03-23 01:26:03] - INFO: Epoch: 12, Batch[20/22], Train loss :1.947, Train acc: 0.531
[2023-03-23 01:26:04] - INFO: Epoch: 12, Train loss: 1.813, Epoch time = 19.353s
[2023-03-23 01:26:05] - INFO: Epoch: 13, Batch[0/22], Train loss :1.252, Train acc: 0.750
[2023-03-23 01:26:14] - INFO: Epoch: 13, Batch[10/22], Train loss :2.143, Train acc: 0.531
[2023-03-23 01:26:23] - INFO: Epoch: 13, Batch[20/22], Train loss :1.986, Train acc: 0.562
[2023-03-23 01:26:23] - INFO: Epoch: 13, Train loss: 1.835, Epoch time = 19.344s
[2023-03-23 01:26:23] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:26:23] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:26:24] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:26:24] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:26:24] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:26:24] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:26:24] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:26:24] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:26:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:26:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:26:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:26:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:26:25] - INFO: Accuracy on val 0.640
[2023-03-23 01:26:26] - INFO: Epoch: 14, Batch[0/22], Train loss :2.083, Train acc: 0.562
[2023-03-23 01:26:35] - INFO: Epoch: 14, Batch[10/22], Train loss :2.242, Train acc: 0.500
[2023-03-23 01:26:44] - INFO: Epoch: 14, Batch[20/22], Train loss :1.494, Train acc: 0.688
[2023-03-23 01:26:44] - INFO: Epoch: 14, Train loss: 1.832, Epoch time = 19.344s
[2023-03-23 01:26:45] - INFO: Epoch: 15, Batch[0/22], Train loss :1.902, Train acc: 0.562
[2023-03-23 01:26:54] - INFO: Epoch: 15, Batch[10/22], Train loss :2.142, Train acc: 0.531
[2023-03-23 01:27:03] - INFO: Epoch: 15, Batch[20/22], Train loss :1.611, Train acc: 0.688
[2023-03-23 01:27:03] - INFO: Epoch: 15, Train loss: 1.795, Epoch time = 19.349s
[2023-03-23 01:27:04] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:27:04] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:27:04] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:27:04] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:27:04] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:27:04] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:27:05] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:27:05] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:27:05] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:27:05] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:27:05] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:27:05] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:27:05] - INFO: Accuracy on val 0.640
[2023-03-23 01:27:06] - INFO: Epoch: 16, Batch[0/22], Train loss :1.986, Train acc: 0.469
[2023-03-23 01:27:15] - INFO: Epoch: 16, Batch[10/22], Train loss :1.679, Train acc: 0.656
[2023-03-23 01:27:24] - INFO: Epoch: 16, Batch[20/22], Train loss :1.420, Train acc: 0.750
[2023-03-23 01:27:24] - INFO: Epoch: 16, Train loss: 1.808, Epoch time = 19.361s
[2023-03-23 01:27:25] - INFO: Epoch: 17, Batch[0/22], Train loss :1.281, Train acc: 0.719
[2023-03-23 01:27:34] - INFO: Epoch: 17, Batch[10/22], Train loss :2.042, Train acc: 0.562
[2023-03-23 01:27:43] - INFO: Epoch: 17, Batch[20/22], Train loss :1.801, Train acc: 0.625
[2023-03-23 01:27:44] - INFO: Epoch: 17, Train loss: 1.810, Epoch time = 19.377s
[2023-03-23 01:27:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:27:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:27:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:27:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:27:45] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:27:45] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:27:45] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:27:45] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:27:45] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:27:45] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:27:45] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:27:45] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:27:45] - INFO: Accuracy on val 0.640
[2023-03-23 01:27:46] - INFO: Epoch: 18, Batch[0/22], Train loss :1.895, Train acc: 0.594
[2023-03-23 01:27:55] - INFO: Epoch: 18, Batch[10/22], Train loss :2.233, Train acc: 0.406
[2023-03-23 01:28:04] - INFO: Epoch: 18, Batch[20/22], Train loss :1.809, Train acc: 0.688
[2023-03-23 01:28:05] - INFO: Epoch: 18, Train loss: 1.797, Epoch time = 19.381s
[2023-03-23 01:28:06] - INFO: Epoch: 19, Batch[0/22], Train loss :1.225, Train acc: 0.844
[2023-03-23 01:28:15] - INFO: Epoch: 19, Batch[10/22], Train loss :1.508, Train acc: 0.719
[2023-03-23 01:28:24] - INFO: Epoch: 19, Batch[20/22], Train loss :1.732, Train acc: 0.625
[2023-03-23 01:28:24] - INFO: Epoch: 19, Train loss: 1.827, Epoch time = 19.399s
[2023-03-23 01:28:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:28:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:28:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:28:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:28:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:28:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:28:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:28:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:28:26] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:28:26] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:28:26] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:28:26] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:28:26] - INFO: Accuracy on val 0.640
[2023-03-23 01:28:27] - INFO: Epoch: 20, Batch[0/22], Train loss :1.981, Train acc: 0.500
[2023-03-23 01:28:36] - INFO: Epoch: 20, Batch[10/22], Train loss :1.959, Train acc: 0.625
[2023-03-23 01:28:45] - INFO: Epoch: 20, Batch[20/22], Train loss :2.135, Train acc: 0.531
[2023-03-23 01:28:45] - INFO: Epoch: 20, Train loss: 1.796, Epoch time = 19.389s
[2023-03-23 01:28:46] - INFO: Epoch: 21, Batch[0/22], Train loss :1.887, Train acc: 0.594
[2023-03-23 01:28:55] - INFO: Epoch: 21, Batch[10/22], Train loss :2.270, Train acc: 0.469
[2023-03-23 01:29:04] - INFO: Epoch: 21, Batch[20/22], Train loss :1.719, Train acc: 0.625
[2023-03-23 01:29:05] - INFO: Epoch: 21, Train loss: 1.791, Epoch time = 19.385s
[2023-03-23 01:29:05] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:29:05] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:29:05] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:29:05] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:29:06] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:29:06] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:29:06] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:29:06] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:29:06] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:29:06] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:29:06] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:29:06] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:29:06] - INFO: Accuracy on val 0.640
[2023-03-23 01:29:07] - INFO: Epoch: 22, Batch[0/22], Train loss :1.166, Train acc: 0.781
[2023-03-23 01:33:33] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 01:33:33] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 01:33:33] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 01:33:33] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 01:33:33] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 01:33:33] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 01:33:33] - INFO: ###  device = cuda:0
[2023-03-23 01:33:33] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 01:33:33] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 01:33:33] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 01:33:33] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 01:33:33] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 01:33:33] - INFO: ###  split_sep = _!_
[2023-03-23 01:33:33] - INFO: ###  is_sample_shuffle = True
[2023-03-23 01:33:33] - INFO: ###  batch_size = 32
[2023-03-23 01:33:33] - INFO: ###  max_sen_len = None
[2023-03-23 01:33:33] - INFO: ###  num_labels = 8191
[2023-03-23 01:33:33] - INFO: ###  epochs = 10
[2023-03-23 01:33:33] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 01:33:33] - INFO: ###  vocab_size = 21128
[2023-03-23 01:33:33] - INFO: ###  hidden_size = 768
[2023-03-23 01:33:33] - INFO: ###  num_hidden_layers = 12
[2023-03-23 01:33:33] - INFO: ###  num_attention_heads = 12
[2023-03-23 01:33:33] - INFO: ###  hidden_act = gelu
[2023-03-23 01:33:33] - INFO: ###  intermediate_size = 3072
[2023-03-23 01:33:33] - INFO: ###  pad_token_id = 0
[2023-03-23 01:33:33] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 01:33:33] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 01:33:33] - INFO: ###  max_position_embeddings = 512
[2023-03-23 01:33:33] - INFO: ###  type_vocab_size = 2
[2023-03-23 01:33:33] - INFO: ###  initializer_range = 0.02
[2023-03-23 01:33:33] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 01:33:33] - INFO: ###  directionality = bidi
[2023-03-23 01:33:33] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 01:33:33] - INFO: ###  model_type = bert
[2023-03-23 01:33:33] - INFO: ###  pooler_fc_size = 768
[2023-03-23 01:33:33] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 01:33:33] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 01:33:33] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 01:33:33] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 01:33:35] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 01:33:36] - INFO: ## 成功载入已有模型，进行追加训练......
[2023-03-23 01:33:38] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/test_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:33:38] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/train_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:33:38] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/val_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:33:40] - INFO: Epoch: 0, Batch[0/22], Train loss :2.073, Train acc: 0.594
[2023-03-23 01:33:49] - INFO: Epoch: 0, Batch[10/22], Train loss :2.089, Train acc: 0.656
[2023-03-23 01:33:57] - INFO: Epoch: 0, Batch[20/22], Train loss :1.284, Train acc: 0.781
[2023-03-23 01:33:58] - INFO: Epoch: 0, Train loss: 1.724, Epoch time = 20.178s
[2023-03-23 01:33:59] - INFO: Epoch: 1, Batch[0/22], Train loss :0.880, Train acc: 0.781
[2023-03-23 01:34:08] - INFO: Epoch: 1, Batch[10/22], Train loss :1.030, Train acc: 0.781
[2023-03-23 01:34:17] - INFO: Epoch: 1, Batch[20/22], Train loss :1.413, Train acc: 0.688
[2023-03-23 01:34:17] - INFO: Epoch: 1, Train loss: 1.289, Epoch time = 19.108s
[2023-03-23 01:34:17] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:34:17] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:34:18] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:34:18] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:34:18] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:34:18] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:34:18] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:34:18] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:34:19] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:34:19] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:34:19] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:34:19] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:34:19] - INFO: Accuracy on val 0.686
[2023-03-23 01:34:20] - INFO: Epoch: 2, Batch[0/22], Train loss :0.964, Train acc: 0.812
[2023-03-23 01:34:29] - INFO: Epoch: 2, Batch[10/22], Train loss :0.830, Train acc: 0.875
[2023-03-23 01:34:38] - INFO: Epoch: 2, Batch[20/22], Train loss :0.810, Train acc: 0.875
[2023-03-23 01:34:39] - INFO: Epoch: 2, Train loss: 1.033, Epoch time = 19.346s
[2023-03-23 01:34:39] - INFO: Epoch: 3, Batch[0/22], Train loss :1.170, Train acc: 0.750
[2023-03-23 01:34:48] - INFO: Epoch: 3, Batch[10/22], Train loss :1.275, Train acc: 0.812
[2023-03-23 01:34:57] - INFO: Epoch: 3, Batch[20/22], Train loss :0.794, Train acc: 0.750
[2023-03-23 01:34:58] - INFO: Epoch: 3, Train loss: 0.892, Epoch time = 19.388s
[2023-03-23 01:34:58] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:34:58] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:34:59] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:34:59] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:34:59] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:34:59] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:34:59] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:34:59] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:34:59] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:34:59] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:35:00] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:35:00] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:35:00] - INFO: Accuracy on val 0.657
[2023-03-23 01:35:00] - INFO: Epoch: 4, Batch[0/22], Train loss :0.614, Train acc: 0.844
[2023-03-23 01:35:09] - INFO: Epoch: 4, Batch[10/22], Train loss :1.158, Train acc: 0.750
[2023-03-23 01:35:18] - INFO: Epoch: 4, Batch[20/22], Train loss :0.583, Train acc: 0.875
[2023-03-23 01:35:19] - INFO: Epoch: 4, Train loss: 0.816, Epoch time = 19.364s
[2023-03-23 01:35:20] - INFO: Epoch: 5, Batch[0/22], Train loss :0.497, Train acc: 0.875
[2023-03-23 01:35:29] - INFO: Epoch: 5, Batch[10/22], Train loss :0.854, Train acc: 0.875
[2023-03-23 01:35:38] - INFO: Epoch: 5, Batch[20/22], Train loss :0.795, Train acc: 0.812
[2023-03-23 01:35:38] - INFO: Epoch: 5, Train loss: 0.675, Epoch time = 19.382s
[2023-03-23 01:35:39] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:35:39] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:35:39] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:35:39] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:35:39] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:35:39] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:35:40] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:35:40] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:35:40] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:35:40] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:35:40] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:35:40] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:35:40] - INFO: Accuracy on val 0.558
[2023-03-23 01:35:41] - INFO: Epoch: 6, Batch[0/22], Train loss :0.666, Train acc: 0.875
[2023-03-23 01:35:50] - INFO: Epoch: 6, Batch[10/22], Train loss :0.768, Train acc: 0.844
[2023-03-23 01:35:59] - INFO: Epoch: 6, Batch[20/22], Train loss :0.490, Train acc: 0.906
[2023-03-23 01:35:59] - INFO: Epoch: 6, Train loss: 0.594, Epoch time = 19.401s
[2023-03-23 01:36:00] - INFO: Epoch: 7, Batch[0/22], Train loss :0.429, Train acc: 0.938
[2023-03-23 01:36:09] - INFO: Epoch: 7, Batch[10/22], Train loss :0.752, Train acc: 0.844
[2023-03-23 01:36:18] - INFO: Epoch: 7, Batch[20/22], Train loss :0.872, Train acc: 0.844
[2023-03-23 01:36:19] - INFO: Epoch: 7, Train loss: 0.561, Epoch time = 19.400s
[2023-03-23 01:36:19] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:36:19] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:36:19] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:36:19] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:36:20] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:36:20] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:36:20] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:36:20] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:36:20] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:36:20] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:36:20] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:36:20] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:36:20] - INFO: Accuracy on val 0.709
[2023-03-23 01:36:22] - INFO: Epoch: 8, Batch[0/22], Train loss :0.402, Train acc: 0.906
[2023-03-23 01:36:31] - INFO: Epoch: 8, Batch[10/22], Train loss :0.606, Train acc: 0.906
[2023-03-23 01:36:40] - INFO: Epoch: 8, Batch[20/22], Train loss :0.709, Train acc: 0.844
[2023-03-23 01:36:40] - INFO: Epoch: 8, Train loss: 0.570, Epoch time = 19.402s
[2023-03-23 01:36:41] - INFO: Epoch: 9, Batch[0/22], Train loss :0.696, Train acc: 0.812
[2023-03-23 01:36:50] - INFO: Epoch: 9, Batch[10/22], Train loss :0.366, Train acc: 0.938
[2023-03-23 01:36:59] - INFO: Epoch: 9, Batch[20/22], Train loss :0.285, Train acc: 0.938
[2023-03-23 01:37:00] - INFO: Epoch: 9, Train loss: 0.415, Epoch time = 19.424s
[2023-03-23 01:37:00] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:37:00] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:37:00] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:37:00] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:37:01] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:37:01] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:37:01] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:37:01] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:37:01] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:37:01] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:37:01] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:37:01] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:37:01] - INFO: Accuracy on val 0.628
[2023-03-23 01:37:04] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 01:37:04] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 01:37:04] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/test_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:37:04] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/train_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:37:04] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/val_None.pt 存在，直接载入缓存文件！
[2023-03-23 01:37:05] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:37:05] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:37:05] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:37:05] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:37:05] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 01:37:05] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 01:37:05] - INFO: Acc on test:0.767
[2023-03-23 17:26:48] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 17:26:48] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 17:26:48] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 17:26:48] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 17:26:48] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 17:26:48] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 17:26:48] - INFO: ###  device = cuda:0
[2023-03-23 17:26:48] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 17:26:48] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 17:26:48] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 17:26:48] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 17:26:48] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 17:26:48] - INFO: ###  split_sep = _!_
[2023-03-23 17:26:48] - INFO: ###  is_sample_shuffle = True
[2023-03-23 17:26:48] - INFO: ###  batch_size = 32
[2023-03-23 17:26:48] - INFO: ###  max_sen_len = None
[2023-03-23 17:26:48] - INFO: ###  num_labels = 8191
[2023-03-23 17:26:48] - INFO: ###  epochs = 10
[2023-03-23 17:26:48] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 17:26:48] - INFO: ###  vocab_size = 21128
[2023-03-23 17:26:48] - INFO: ###  hidden_size = 768
[2023-03-23 17:26:48] - INFO: ###  num_hidden_layers = 12
[2023-03-23 17:26:48] - INFO: ###  num_attention_heads = 12
[2023-03-23 17:26:48] - INFO: ###  hidden_act = gelu
[2023-03-23 17:26:48] - INFO: ###  intermediate_size = 3072
[2023-03-23 17:26:48] - INFO: ###  pad_token_id = 0
[2023-03-23 17:26:48] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 17:26:48] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 17:26:48] - INFO: ###  max_position_embeddings = 512
[2023-03-23 17:26:48] - INFO: ###  type_vocab_size = 2
[2023-03-23 17:26:48] - INFO: ###  initializer_range = 0.02
[2023-03-23 17:26:48] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 17:26:48] - INFO: ###  directionality = bidi
[2023-03-23 17:26:48] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 17:26:48] - INFO: ###  model_type = bert
[2023-03-23 17:26:48] - INFO: ###  pooler_fc_size = 768
[2023-03-23 17:26:48] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 17:26:48] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 17:26:48] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 17:26:48] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 17:27:55] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 17:27:55] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 17:27:55] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 17:27:55] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 17:27:55] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 17:27:55] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 17:27:55] - INFO: ###  device = cuda:0
[2023-03-23 17:27:55] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 17:27:55] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 17:27:55] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 17:27:55] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 17:27:55] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 17:27:55] - INFO: ###  split_sep = _!_
[2023-03-23 17:27:55] - INFO: ###  is_sample_shuffle = True
[2023-03-23 17:27:55] - INFO: ###  batch_size = 32
[2023-03-23 17:27:55] - INFO: ###  max_sen_len = None
[2023-03-23 17:27:55] - INFO: ###  num_labels = 8191
[2023-03-23 17:27:55] - INFO: ###  epochs = 10
[2023-03-23 17:27:55] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 17:27:55] - INFO: ###  vocab_size = 21128
[2023-03-23 17:27:55] - INFO: ###  hidden_size = 768
[2023-03-23 17:27:55] - INFO: ###  num_hidden_layers = 12
[2023-03-23 17:27:55] - INFO: ###  num_attention_heads = 12
[2023-03-23 17:27:55] - INFO: ###  hidden_act = gelu
[2023-03-23 17:27:55] - INFO: ###  intermediate_size = 3072
[2023-03-23 17:27:55] - INFO: ###  pad_token_id = 0
[2023-03-23 17:27:55] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 17:27:55] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 17:27:55] - INFO: ###  max_position_embeddings = 512
[2023-03-23 17:27:55] - INFO: ###  type_vocab_size = 2
[2023-03-23 17:27:55] - INFO: ###  initializer_range = 0.02
[2023-03-23 17:27:55] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 17:27:55] - INFO: ###  directionality = bidi
[2023-03-23 17:27:55] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 17:27:55] - INFO: ###  model_type = bert
[2023-03-23 17:27:55] - INFO: ###  pooler_fc_size = 768
[2023-03-23 17:27:55] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 17:27:55] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 17:27:55] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 17:27:55] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 17:29:04] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 17:30:11] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 17:50:54] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 17:50:54] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 17:50:54] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 17:50:54] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 17:50:54] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 17:50:54] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 17:50:54] - INFO: ###  device = cuda:0
[2023-03-23 17:50:54] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 17:50:54] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 17:50:54] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 17:50:54] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 17:50:54] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 17:50:54] - INFO: ###  split_sep = _!_
[2023-03-23 17:50:54] - INFO: ###  is_sample_shuffle = True
[2023-03-23 17:50:54] - INFO: ###  batch_size = 32
[2023-03-23 17:50:54] - INFO: ###  max_sen_len = None
[2023-03-23 17:50:54] - INFO: ###  num_labels = 8191
[2023-03-23 17:50:54] - INFO: ###  epochs = 10
[2023-03-23 17:50:54] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 17:50:54] - INFO: ###  vocab_size = 21128
[2023-03-23 17:50:54] - INFO: ###  hidden_size = 768
[2023-03-23 17:50:54] - INFO: ###  num_hidden_layers = 12
[2023-03-23 17:50:54] - INFO: ###  num_attention_heads = 12
[2023-03-23 17:50:54] - INFO: ###  hidden_act = gelu
[2023-03-23 17:50:54] - INFO: ###  intermediate_size = 3072
[2023-03-23 17:50:54] - INFO: ###  pad_token_id = 0
[2023-03-23 17:50:54] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 17:50:54] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 17:50:54] - INFO: ###  max_position_embeddings = 512
[2023-03-23 17:50:54] - INFO: ###  type_vocab_size = 2
[2023-03-23 17:50:54] - INFO: ###  initializer_range = 0.02
[2023-03-23 17:50:54] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 17:50:54] - INFO: ###  directionality = bidi
[2023-03-23 17:50:54] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 17:50:54] - INFO: ###  model_type = bert
[2023-03-23 17:50:54] - INFO: ###  pooler_fc_size = 768
[2023-03-23 17:50:54] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 17:50:54] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 17:50:54] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 17:50:54] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 17:50:56] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 17:50:59] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 17:50:59] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/test_None.pt 存在，直接载入缓存文件！
[2023-03-23 17:50:59] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/train_None.pt 存在，直接载入缓存文件！
[2023-03-23 17:50:59] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/val_None.pt 存在，直接载入缓存文件！
[2023-03-23 17:51:00] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 17:51:00] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 17:51:00] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 17:51:00] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 17:51:00] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 17:51:00] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 17:51:00] - INFO: Acc on test:0.753
[2023-03-23 17:58:10] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 17:58:10] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 17:58:10] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 17:58:10] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 17:58:10] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 17:58:10] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 17:58:10] - INFO: ###  device = cuda:0
[2023-03-23 17:58:10] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 17:58:10] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 17:58:10] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 17:58:10] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 17:58:10] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 17:58:10] - INFO: ###  split_sep = _!_
[2023-03-23 17:58:10] - INFO: ###  is_sample_shuffle = True
[2023-03-23 17:58:10] - INFO: ###  batch_size = 32
[2023-03-23 17:58:10] - INFO: ###  max_sen_len = None
[2023-03-23 17:58:10] - INFO: ###  num_labels = 8191
[2023-03-23 17:58:10] - INFO: ###  epochs = 10
[2023-03-23 17:58:10] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 17:58:10] - INFO: ###  vocab_size = 21128
[2023-03-23 17:58:10] - INFO: ###  hidden_size = 768
[2023-03-23 17:58:10] - INFO: ###  num_hidden_layers = 12
[2023-03-23 17:58:10] - INFO: ###  num_attention_heads = 12
[2023-03-23 17:58:10] - INFO: ###  hidden_act = gelu
[2023-03-23 17:58:10] - INFO: ###  intermediate_size = 3072
[2023-03-23 17:58:10] - INFO: ###  pad_token_id = 0
[2023-03-23 17:58:10] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 17:58:10] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 17:58:10] - INFO: ###  max_position_embeddings = 512
[2023-03-23 17:58:10] - INFO: ###  type_vocab_size = 2
[2023-03-23 17:58:10] - INFO: ###  initializer_range = 0.02
[2023-03-23 17:58:10] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 17:58:10] - INFO: ###  directionality = bidi
[2023-03-23 17:58:10] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 17:58:10] - INFO: ###  model_type = bert
[2023-03-23 17:58:10] - INFO: ###  pooler_fc_size = 768
[2023-03-23 17:58:10] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 17:58:10] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 17:58:10] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 17:58:10] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 18:02:56] - INFO: 缓存文件 _None.pt 不存在，重新处理并缓存！
[2023-03-23 18:03:22] - INFO: 缓存文件 _None.pt 存在，直接载入缓存文件！
[2023-03-23 19:03:01] - INFO: 缓存文件 _None.pt 存在，直接载入缓存文件！
[2023-03-23 19:03:39] - INFO: 缓存文件 _None.pt 存在，直接载入缓存文件！
[2023-03-23 19:08:31] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 19:08:31] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 19:08:31] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 19:08:31] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 19:08:31] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 19:08:31] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 19:08:31] - INFO: ###  device = cuda:0
[2023-03-23 19:08:31] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 19:08:31] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 19:08:31] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 19:08:31] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 19:08:31] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 19:08:31] - INFO: ###  split_sep = _!_
[2023-03-23 19:08:31] - INFO: ###  is_sample_shuffle = True
[2023-03-23 19:08:31] - INFO: ###  batch_size = 32
[2023-03-23 19:08:31] - INFO: ###  max_sen_len = None
[2023-03-23 19:08:31] - INFO: ###  num_labels = 8191
[2023-03-23 19:08:31] - INFO: ###  epochs = 10
[2023-03-23 19:08:31] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 19:08:31] - INFO: ###  max_position_embeddings = 512
[2023-03-23 19:08:31] - INFO: ###  vocab_size = 21128
[2023-03-23 19:08:31] - INFO: ###  hidden_size = 768
[2023-03-23 19:08:31] - INFO: ###  num_hidden_layers = 12
[2023-03-23 19:08:31] - INFO: ###  num_attention_heads = 12
[2023-03-23 19:08:31] - INFO: ###  hidden_act = gelu
[2023-03-23 19:08:31] - INFO: ###  intermediate_size = 3072
[2023-03-23 19:08:31] - INFO: ###  pad_token_id = 0
[2023-03-23 19:08:31] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 19:08:31] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 19:08:31] - INFO: ###  type_vocab_size = 2
[2023-03-23 19:08:31] - INFO: ###  initializer_range = 0.02
[2023-03-23 19:08:31] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 19:08:31] - INFO: ###  directionality = bidi
[2023-03-23 19:08:31] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 19:08:31] - INFO: ###  model_type = bert
[2023-03-23 19:08:31] - INFO: ###  pooler_fc_size = 768
[2023-03-23 19:08:31] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 19:08:31] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 19:08:31] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 19:08:31] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 19:08:33] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 19:08:35] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 19:08:35] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/test_None.pt 不存在，重新处理并缓存！
[2023-03-23 19:08:36] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/train_None.pt 不存在，重新处理并缓存！
[2023-03-23 19:08:38] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/val_None.pt 不存在，重新处理并缓存！
[2023-03-23 19:08:40] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:08:40] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:08:40] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:08:40] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:08:40] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:08:40] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:08:40] - INFO: Acc on test:0.753
[2023-03-23 19:09:22] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 19:09:22] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 19:09:22] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 19:09:22] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 19:09:22] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 19:09:22] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 19:09:22] - INFO: ###  device = cuda:0
[2023-03-23 19:09:22] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 19:09:22] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 19:09:22] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 19:09:22] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 19:09:22] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 19:09:22] - INFO: ###  split_sep = _!_
[2023-03-23 19:09:22] - INFO: ###  is_sample_shuffle = True
[2023-03-23 19:09:22] - INFO: ###  batch_size = 32
[2023-03-23 19:09:22] - INFO: ###  max_sen_len = None
[2023-03-23 19:09:22] - INFO: ###  num_labels = 8191
[2023-03-23 19:09:22] - INFO: ###  epochs = 10
[2023-03-23 19:09:22] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 19:09:22] - INFO: ###  max_position_embeddings = 512
[2023-03-23 19:09:22] - INFO: ###  vocab_size = 21128
[2023-03-23 19:09:22] - INFO: ###  hidden_size = 768
[2023-03-23 19:09:22] - INFO: ###  num_hidden_layers = 12
[2023-03-23 19:09:22] - INFO: ###  num_attention_heads = 12
[2023-03-23 19:09:22] - INFO: ###  hidden_act = gelu
[2023-03-23 19:09:22] - INFO: ###  intermediate_size = 3072
[2023-03-23 19:09:22] - INFO: ###  pad_token_id = 0
[2023-03-23 19:09:22] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 19:09:22] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 19:09:22] - INFO: ###  type_vocab_size = 2
[2023-03-23 19:09:22] - INFO: ###  initializer_range = 0.02
[2023-03-23 19:09:22] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 19:09:22] - INFO: ###  directionality = bidi
[2023-03-23 19:09:22] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 19:09:22] - INFO: ###  model_type = bert
[2023-03-23 19:09:22] - INFO: ###  pooler_fc_size = 768
[2023-03-23 19:09:22] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 19:09:22] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 19:09:22] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 19:09:22] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 19:09:24] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 19:09:26] - INFO: ## 成功载入已有模型，进行追加训练......
[2023-03-23 19:09:26] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/test_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:09:26] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/train_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:09:26] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/val_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:09:28] - INFO: Epoch: 0, Batch[0/22], Train loss :0.504, Train acc: 0.906
[2023-03-23 19:09:37] - INFO: Epoch: 0, Batch[10/22], Train loss :0.378, Train acc: 0.906
[2023-03-23 19:09:46] - INFO: Epoch: 0, Batch[20/22], Train loss :0.356, Train acc: 0.938
[2023-03-23 19:09:46] - INFO: Epoch: 0, Train loss: 0.422, Epoch time = 19.991s
[2023-03-23 19:09:47] - INFO: Epoch: 1, Batch[0/22], Train loss :0.304, Train acc: 0.938
[2023-03-23 19:09:56] - INFO: Epoch: 1, Batch[10/22], Train loss :0.391, Train acc: 0.938
[2023-03-23 19:10:05] - INFO: Epoch: 1, Batch[20/22], Train loss :0.136, Train acc: 0.969
[2023-03-23 19:10:05] - INFO: Epoch: 1, Train loss: 0.356, Epoch time = 19.177s
[2023-03-23 19:10:06] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:10:06] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:10:06] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:10:06] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:10:06] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:10:06] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:10:07] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:10:07] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:10:07] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:10:07] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:10:07] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:10:07] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:10:07] - INFO: Accuracy on val 0.640
[2023-03-23 19:10:08] - INFO: Epoch: 2, Batch[0/22], Train loss :0.173, Train acc: 0.969
[2023-03-23 19:10:17] - INFO: Epoch: 2, Batch[10/22], Train loss :0.213, Train acc: 0.938
[2023-03-23 19:10:26] - INFO: Epoch: 2, Batch[20/22], Train loss :0.465, Train acc: 0.906
[2023-03-23 19:10:27] - INFO: Epoch: 2, Train loss: 0.327, Epoch time = 19.266s
[2023-03-23 19:10:28] - INFO: Epoch: 3, Batch[0/22], Train loss :0.310, Train acc: 0.938
[2023-03-23 19:10:37] - INFO: Epoch: 3, Batch[10/22], Train loss :0.139, Train acc: 0.969
[2023-03-23 19:10:46] - INFO: Epoch: 3, Batch[20/22], Train loss :0.244, Train acc: 0.969
[2023-03-23 19:10:46] - INFO: Epoch: 3, Train loss: 0.255, Epoch time = 19.325s
[2023-03-23 19:10:46] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:10:46] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:10:47] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:10:47] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:10:47] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:10:47] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:10:47] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:10:47] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:10:48] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:10:48] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:10:48] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:10:48] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:10:48] - INFO: Accuracy on val 0.663
[2023-03-23 19:10:49] - INFO: Epoch: 4, Batch[0/22], Train loss :0.336, Train acc: 0.906
[2023-03-23 19:10:58] - INFO: Epoch: 4, Batch[10/22], Train loss :0.060, Train acc: 1.000
[2023-03-23 19:11:07] - INFO: Epoch: 4, Batch[20/22], Train loss :0.297, Train acc: 0.938
[2023-03-23 19:11:08] - INFO: Epoch: 4, Train loss: 0.201, Epoch time = 19.314s
[2023-03-23 19:11:08] - INFO: Epoch: 5, Batch[0/22], Train loss :0.408, Train acc: 0.906
[2023-03-23 19:11:17] - INFO: Epoch: 5, Batch[10/22], Train loss :0.163, Train acc: 0.969
[2023-03-23 19:11:26] - INFO: Epoch: 5, Batch[20/22], Train loss :0.149, Train acc: 0.969
[2023-03-23 19:11:27] - INFO: Epoch: 5, Train loss: 0.175, Epoch time = 19.329s
[2023-03-23 19:11:27] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:11:27] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:11:27] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:11:27] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:11:28] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:11:28] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:11:28] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:11:28] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:11:28] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:11:28] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:11:29] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:11:29] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:11:29] - INFO: Accuracy on val 0.680
[2023-03-23 19:11:30] - INFO: Epoch: 6, Batch[0/22], Train loss :0.029, Train acc: 1.000
[2023-03-23 19:11:39] - INFO: Epoch: 6, Batch[10/22], Train loss :0.333, Train acc: 0.906
[2023-03-23 19:11:48] - INFO: Epoch: 6, Batch[20/22], Train loss :0.155, Train acc: 0.969
[2023-03-23 19:11:49] - INFO: Epoch: 6, Train loss: 0.161, Epoch time = 19.358s
[2023-03-23 19:11:49] - INFO: Epoch: 7, Batch[0/22], Train loss :0.189, Train acc: 0.969
[2023-03-23 19:11:58] - INFO: Epoch: 7, Batch[10/22], Train loss :0.014, Train acc: 1.000
[2023-03-23 19:12:07] - INFO: Epoch: 7, Batch[20/22], Train loss :0.311, Train acc: 0.969
[2023-03-23 19:12:08] - INFO: Epoch: 7, Train loss: 0.122, Epoch time = 19.362s
[2023-03-23 19:12:08] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:12:08] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:12:08] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:12:08] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:12:09] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:12:09] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:12:09] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:12:09] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:12:09] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:12:09] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:12:10] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:12:10] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:12:10] - INFO: Accuracy on val 0.686
[2023-03-23 19:12:11] - INFO: Epoch: 8, Batch[0/22], Train loss :0.166, Train acc: 0.969
[2023-03-23 19:12:20] - INFO: Epoch: 8, Batch[10/22], Train loss :0.025, Train acc: 1.000
[2023-03-23 19:12:29] - INFO: Epoch: 8, Batch[20/22], Train loss :0.144, Train acc: 0.969
[2023-03-23 19:12:30] - INFO: Epoch: 8, Train loss: 0.123, Epoch time = 19.364s
[2023-03-23 19:12:31] - INFO: Epoch: 9, Batch[0/22], Train loss :0.082, Train acc: 1.000
[2023-03-23 19:12:40] - INFO: Epoch: 9, Batch[10/22], Train loss :0.043, Train acc: 1.000
[2023-03-23 19:12:49] - INFO: Epoch: 9, Batch[20/22], Train loss :0.049, Train acc: 1.000
[2023-03-23 19:12:49] - INFO: Epoch: 9, Train loss: 0.096, Epoch time = 19.387s
[2023-03-23 19:12:49] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:12:49] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:12:50] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:12:50] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:12:50] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:12:50] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:12:50] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:12:50] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:12:51] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:12:51] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:12:51] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:12:51] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:12:51] - INFO: Accuracy on val 0.663
[2023-03-23 19:12:52] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 19:12:53] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 19:12:53] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/test_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:12:53] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/train_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:12:53] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/val_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:12:53] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:12:53] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:12:54] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:12:54] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:12:54] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:12:54] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:12:54] - INFO: Acc on test:0.753
[2023-03-23 19:14:15] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 19:14:15] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 19:14:15] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 19:14:15] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 19:14:15] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 19:14:15] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 19:14:15] - INFO: ###  device = cuda:0
[2023-03-23 19:14:15] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 19:14:15] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 19:14:15] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 19:14:15] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 19:14:15] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 19:14:15] - INFO: ###  split_sep = _!_
[2023-03-23 19:14:15] - INFO: ###  is_sample_shuffle = True
[2023-03-23 19:14:15] - INFO: ###  batch_size = 32
[2023-03-23 19:14:15] - INFO: ###  max_sen_len = None
[2023-03-23 19:14:15] - INFO: ###  num_labels = 8191
[2023-03-23 19:14:15] - INFO: ###  epochs = 20
[2023-03-23 19:14:15] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 19:14:15] - INFO: ###  max_position_embeddings = 512
[2023-03-23 19:14:15] - INFO: ###  vocab_size = 21128
[2023-03-23 19:14:15] - INFO: ###  hidden_size = 768
[2023-03-23 19:14:15] - INFO: ###  num_hidden_layers = 12
[2023-03-23 19:14:15] - INFO: ###  num_attention_heads = 12
[2023-03-23 19:14:15] - INFO: ###  hidden_act = gelu
[2023-03-23 19:14:15] - INFO: ###  intermediate_size = 3072
[2023-03-23 19:14:15] - INFO: ###  pad_token_id = 0
[2023-03-23 19:14:15] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 19:14:15] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 19:14:15] - INFO: ###  type_vocab_size = 2
[2023-03-23 19:14:15] - INFO: ###  initializer_range = 0.02
[2023-03-23 19:14:15] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 19:14:15] - INFO: ###  directionality = bidi
[2023-03-23 19:14:15] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 19:14:15] - INFO: ###  model_type = bert
[2023-03-23 19:14:15] - INFO: ###  pooler_fc_size = 768
[2023-03-23 19:14:15] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 19:14:15] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 19:14:15] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 19:14:15] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 19:14:17] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 19:14:18] - INFO: ## 成功载入已有模型，进行追加训练......
[2023-03-23 19:14:19] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/test_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:14:19] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/train_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:14:19] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/val_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:14:21] - INFO: Epoch: 0, Batch[0/22], Train loss :2.430, Train acc: 0.531
[2023-03-23 19:14:30] - INFO: Epoch: 0, Batch[10/22], Train loss :1.112, Train acc: 0.750
[2023-03-23 19:14:39] - INFO: Epoch: 0, Batch[20/22], Train loss :1.532, Train acc: 0.656
[2023-03-23 19:14:39] - INFO: Epoch: 0, Train loss: 1.658, Epoch time = 19.972s
[2023-03-23 19:14:40] - INFO: Epoch: 1, Batch[0/22], Train loss :1.043, Train acc: 0.812
[2023-03-23 19:14:49] - INFO: Epoch: 1, Batch[10/22], Train loss :0.788, Train acc: 0.781
[2023-03-23 19:14:58] - INFO: Epoch: 1, Batch[20/22], Train loss :0.892, Train acc: 0.844
[2023-03-23 19:14:58] - INFO: Epoch: 1, Train loss: 1.295, Epoch time = 19.244s
[2023-03-23 19:14:59] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:14:59] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:14:59] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:14:59] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:14:59] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:14:59] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:15:00] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:15:00] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:15:00] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:15:00] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:15:00] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:15:00] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:15:00] - INFO: Accuracy on val 0.564
[2023-03-23 19:15:01] - INFO: Epoch: 2, Batch[0/22], Train loss :1.109, Train acc: 0.719
[2023-03-23 19:15:10] - INFO: Epoch: 2, Batch[10/22], Train loss :1.654, Train acc: 0.656
[2023-03-23 19:15:19] - INFO: Epoch: 2, Batch[20/22], Train loss :0.560, Train acc: 0.875
[2023-03-23 19:15:20] - INFO: Epoch: 2, Train loss: 1.083, Epoch time = 19.276s
[2023-03-23 19:15:21] - INFO: Epoch: 3, Batch[0/22], Train loss :1.169, Train acc: 0.750
[2023-03-23 19:15:30] - INFO: Epoch: 3, Batch[10/22], Train loss :0.790, Train acc: 0.812
[2023-03-23 19:15:39] - INFO: Epoch: 3, Batch[20/22], Train loss :0.449, Train acc: 0.906
[2023-03-23 19:15:39] - INFO: Epoch: 3, Train loss: 0.875, Epoch time = 19.356s
[2023-03-23 19:15:40] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:15:40] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:15:40] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:15:40] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:15:40] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:15:40] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:15:40] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:15:40] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:15:41] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:15:41] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:15:41] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:15:41] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:15:41] - INFO: Accuracy on val 0.686
[2023-03-23 19:15:42] - INFO: Epoch: 4, Batch[0/22], Train loss :0.913, Train acc: 0.719
[2023-03-23 19:15:51] - INFO: Epoch: 4, Batch[10/22], Train loss :0.468, Train acc: 0.906
[2023-03-23 19:16:00] - INFO: Epoch: 4, Batch[20/22], Train loss :0.911, Train acc: 0.875
[2023-03-23 19:16:01] - INFO: Epoch: 4, Train loss: 0.744, Epoch time = 19.407s
[2023-03-23 19:16:02] - INFO: Epoch: 5, Batch[0/22], Train loss :0.913, Train acc: 0.812
[2023-03-23 19:16:11] - INFO: Epoch: 5, Batch[10/22], Train loss :0.297, Train acc: 1.000
[2023-03-23 19:16:20] - INFO: Epoch: 5, Batch[20/22], Train loss :0.457, Train acc: 0.875
[2023-03-23 19:16:20] - INFO: Epoch: 5, Train loss: 0.667, Epoch time = 19.436s
[2023-03-23 19:16:20] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:16:20] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:16:21] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:16:21] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:16:21] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:16:21] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:16:21] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:16:21] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:16:22] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:16:22] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:16:22] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:16:22] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:16:22] - INFO: Accuracy on val 0.622
[2023-03-23 19:16:23] - INFO: Epoch: 6, Batch[0/22], Train loss :0.231, Train acc: 1.000
[2023-03-23 19:16:32] - INFO: Epoch: 6, Batch[10/22], Train loss :0.315, Train acc: 0.906
[2023-03-23 19:16:41] - INFO: Epoch: 6, Batch[20/22], Train loss :0.526, Train acc: 0.875
[2023-03-23 19:16:41] - INFO: Epoch: 6, Train loss: 0.580, Epoch time = 19.455s
[2023-03-23 19:16:42] - INFO: Epoch: 7, Batch[0/22], Train loss :0.371, Train acc: 0.969
[2023-03-23 19:16:51] - INFO: Epoch: 7, Batch[10/22], Train loss :0.690, Train acc: 0.844
[2023-03-23 19:17:00] - INFO: Epoch: 7, Batch[20/22], Train loss :0.392, Train acc: 0.938
[2023-03-23 19:17:01] - INFO: Epoch: 7, Train loss: 0.474, Epoch time = 19.467s
[2023-03-23 19:17:01] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:17:01] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:17:01] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:17:01] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:17:02] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:17:02] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:17:02] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:17:02] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:17:02] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:17:02] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:17:02] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:17:02] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:17:02] - INFO: Accuracy on val 0.645
[2023-03-23 19:17:03] - INFO: Epoch: 8, Batch[0/22], Train loss :0.410, Train acc: 0.906
[2023-03-23 19:17:12] - INFO: Epoch: 8, Batch[10/22], Train loss :0.240, Train acc: 0.969
[2023-03-23 19:17:21] - INFO: Epoch: 8, Batch[20/22], Train loss :0.092, Train acc: 1.000
[2023-03-23 19:17:22] - INFO: Epoch: 8, Train loss: 0.380, Epoch time = 19.482s
[2023-03-23 19:17:23] - INFO: Epoch: 9, Batch[0/22], Train loss :0.271, Train acc: 0.906
[2023-03-23 19:17:32] - INFO: Epoch: 9, Batch[10/22], Train loss :0.653, Train acc: 0.875
[2023-03-23 19:17:41] - INFO: Epoch: 9, Batch[20/22], Train loss :0.372, Train acc: 0.938
[2023-03-23 19:17:41] - INFO: Epoch: 9, Train loss: 0.315, Epoch time = 19.474s
[2023-03-23 19:17:42] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:17:42] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:17:42] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:17:42] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:17:42] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:17:42] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:17:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:17:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:17:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:17:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:17:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:17:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:17:43] - INFO: Accuracy on val 0.669
[2023-03-23 19:17:44] - INFO: Epoch: 10, Batch[0/22], Train loss :0.362, Train acc: 0.906
[2023-03-23 19:17:53] - INFO: Epoch: 10, Batch[10/22], Train loss :0.179, Train acc: 0.969
[2023-03-23 19:18:02] - INFO: Epoch: 10, Batch[20/22], Train loss :0.385, Train acc: 0.875
[2023-03-23 19:18:03] - INFO: Epoch: 10, Train loss: 0.326, Epoch time = 19.513s
[2023-03-23 19:18:03] - INFO: Epoch: 11, Batch[0/22], Train loss :0.536, Train acc: 0.875
[2023-03-23 19:18:13] - INFO: Epoch: 11, Batch[10/22], Train loss :0.210, Train acc: 0.938
[2023-03-23 19:18:22] - INFO: Epoch: 11, Batch[20/22], Train loss :0.319, Train acc: 0.969
[2023-03-23 19:18:22] - INFO: Epoch: 11, Train loss: 0.265, Epoch time = 19.503s
[2023-03-23 19:18:22] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:18:22] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:18:23] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:18:23] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:18:23] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:18:23] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:18:23] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:18:23] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:18:24] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:18:24] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:18:24] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:18:24] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:18:24] - INFO: Accuracy on val 0.657
[2023-03-23 19:18:25] - INFO: Epoch: 12, Batch[0/22], Train loss :0.119, Train acc: 1.000
[2023-03-23 19:18:34] - INFO: Epoch: 12, Batch[10/22], Train loss :0.288, Train acc: 0.969
[2023-03-23 19:18:43] - INFO: Epoch: 12, Batch[20/22], Train loss :0.213, Train acc: 0.969
[2023-03-23 19:18:43] - INFO: Epoch: 12, Train loss: 0.229, Epoch time = 19.432s
[2023-03-23 19:18:44] - INFO: Epoch: 13, Batch[0/22], Train loss :0.223, Train acc: 0.938
[2023-03-23 19:18:53] - INFO: Epoch: 13, Batch[10/22], Train loss :0.166, Train acc: 1.000
[2023-03-23 19:19:02] - INFO: Epoch: 13, Batch[20/22], Train loss :0.352, Train acc: 0.906
[2023-03-23 19:19:03] - INFO: Epoch: 13, Train loss: 0.292, Epoch time = 19.418s
[2023-03-23 19:19:03] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:19:03] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:19:03] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:19:03] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:19:04] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:19:04] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:19:04] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:19:04] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:19:04] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:19:04] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:19:04] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:19:04] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:19:04] - INFO: Accuracy on val 0.622
[2023-03-23 19:19:05] - INFO: Epoch: 14, Batch[0/22], Train loss :0.349, Train acc: 0.938
[2023-03-23 19:19:14] - INFO: Epoch: 14, Batch[10/22], Train loss :0.468, Train acc: 0.875
[2023-03-23 19:19:23] - INFO: Epoch: 14, Batch[20/22], Train loss :0.383, Train acc: 0.906
[2023-03-23 19:19:24] - INFO: Epoch: 14, Train loss: 0.262, Epoch time = 19.425s
[2023-03-23 19:19:25] - INFO: Epoch: 15, Batch[0/22], Train loss :0.156, Train acc: 0.969
[2023-03-23 19:19:34] - INFO: Epoch: 15, Batch[10/22], Train loss :0.222, Train acc: 0.938
[2023-03-23 19:19:43] - INFO: Epoch: 15, Batch[20/22], Train loss :0.071, Train acc: 1.000
[2023-03-23 19:19:43] - INFO: Epoch: 15, Train loss: 0.212, Epoch time = 19.419s
[2023-03-23 19:19:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:19:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:19:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:19:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:19:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:19:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:19:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:19:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:19:45] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:19:45] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:19:45] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:19:45] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:19:45] - INFO: Accuracy on val 0.651
[2023-03-23 19:19:46] - INFO: Epoch: 16, Batch[0/22], Train loss :0.229, Train acc: 0.969
[2023-03-23 19:19:55] - INFO: Epoch: 16, Batch[10/22], Train loss :0.199, Train acc: 0.969
[2023-03-23 19:20:04] - INFO: Epoch: 16, Batch[20/22], Train loss :0.178, Train acc: 0.938
[2023-03-23 19:20:04] - INFO: Epoch: 16, Train loss: 0.177, Epoch time = 19.413s
[2023-03-23 19:20:05] - INFO: Epoch: 17, Batch[0/22], Train loss :0.077, Train acc: 0.969
[2023-03-23 19:20:14] - INFO: Epoch: 17, Batch[10/22], Train loss :0.028, Train acc: 1.000
[2023-03-23 19:20:23] - INFO: Epoch: 17, Batch[20/22], Train loss :0.307, Train acc: 0.969
[2023-03-23 19:20:24] - INFO: Epoch: 17, Train loss: 0.183, Epoch time = 19.383s
[2023-03-23 19:20:24] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:20:24] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:20:24] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:20:24] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:20:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:20:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:20:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:20:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:20:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:20:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:20:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:20:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:20:25] - INFO: Accuracy on val 0.523
[2023-03-23 19:20:26] - INFO: Epoch: 18, Batch[0/22], Train loss :0.519, Train acc: 0.906
[2023-03-23 19:20:35] - INFO: Epoch: 18, Batch[10/22], Train loss :0.172, Train acc: 0.969
[2023-03-23 19:20:44] - INFO: Epoch: 18, Batch[20/22], Train loss :0.314, Train acc: 0.906
[2023-03-23 19:20:45] - INFO: Epoch: 18, Train loss: 0.260, Epoch time = 19.401s
[2023-03-23 19:20:46] - INFO: Epoch: 19, Batch[0/22], Train loss :0.045, Train acc: 1.000
[2023-03-23 19:20:55] - INFO: Epoch: 19, Batch[10/22], Train loss :0.157, Train acc: 0.938
[2023-03-23 19:21:04] - INFO: Epoch: 19, Batch[20/22], Train loss :0.190, Train acc: 0.969
[2023-03-23 19:21:04] - INFO: Epoch: 19, Train loss: 0.165, Epoch time = 19.383s
[2023-03-23 19:21:04] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:21:04] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:21:05] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:21:05] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:21:05] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:21:05] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:21:05] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:21:05] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:21:06] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:21:06] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:21:06] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:21:06] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:21:06] - INFO: Accuracy on val 0.680
[2023-03-23 19:21:08] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 19:21:08] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 19:21:08] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/test_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:21:08] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/train_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:21:08] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/val_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:21:09] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:21:09] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:21:09] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:21:09] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:21:09] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:21:09] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:21:09] - INFO: Acc on test:0.685
[2023-03-23 19:21:55] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 19:21:55] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 19:21:55] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 19:21:55] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 19:21:55] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 19:21:55] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 19:21:55] - INFO: ###  device = cuda:0
[2023-03-23 19:21:55] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 19:21:55] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 19:21:55] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 19:21:55] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 19:21:55] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 19:21:55] - INFO: ###  split_sep = _!_
[2023-03-23 19:21:55] - INFO: ###  is_sample_shuffle = True
[2023-03-23 19:21:55] - INFO: ###  batch_size = 32
[2023-03-23 19:21:55] - INFO: ###  max_sen_len = None
[2023-03-23 19:21:55] - INFO: ###  num_labels = 8191
[2023-03-23 19:21:55] - INFO: ###  epochs = 16
[2023-03-23 19:21:55] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 19:21:55] - INFO: ###  max_position_embeddings = 512
[2023-03-23 19:21:55] - INFO: ###  vocab_size = 21128
[2023-03-23 19:21:55] - INFO: ###  hidden_size = 768
[2023-03-23 19:21:55] - INFO: ###  num_hidden_layers = 12
[2023-03-23 19:21:55] - INFO: ###  num_attention_heads = 12
[2023-03-23 19:21:55] - INFO: ###  hidden_act = gelu
[2023-03-23 19:21:55] - INFO: ###  intermediate_size = 3072
[2023-03-23 19:21:55] - INFO: ###  pad_token_id = 0
[2023-03-23 19:21:55] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 19:21:55] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 19:21:55] - INFO: ###  type_vocab_size = 2
[2023-03-23 19:21:55] - INFO: ###  initializer_range = 0.02
[2023-03-23 19:21:55] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 19:21:55] - INFO: ###  directionality = bidi
[2023-03-23 19:21:55] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 19:21:55] - INFO: ###  model_type = bert
[2023-03-23 19:21:55] - INFO: ###  pooler_fc_size = 768
[2023-03-23 19:21:55] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 19:21:55] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 19:21:55] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 19:21:55] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 19:21:57] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 19:21:59] - INFO: ## 成功载入已有模型，进行追加训练......
[2023-03-23 19:22:00] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/test_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:22:00] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/train_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:22:00] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/val_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:22:02] - INFO: Epoch: 0, Batch[0/22], Train loss :0.630, Train acc: 0.844
[2023-03-23 19:22:10] - INFO: Epoch: 0, Batch[10/22], Train loss :0.590, Train acc: 0.906
[2023-03-23 19:22:19] - INFO: Epoch: 0, Batch[20/22], Train loss :0.817, Train acc: 0.844
[2023-03-23 19:22:20] - INFO: Epoch: 0, Train loss: 0.726, Epoch time = 20.084s
[2023-03-23 19:22:21] - INFO: Epoch: 1, Batch[0/22], Train loss :0.895, Train acc: 0.875
[2023-03-23 19:22:30] - INFO: Epoch: 1, Batch[10/22], Train loss :0.512, Train acc: 0.875
[2023-03-23 19:22:39] - INFO: Epoch: 1, Batch[20/22], Train loss :1.066, Train acc: 0.750
[2023-03-23 19:22:39] - INFO: Epoch: 1, Train loss: 0.672, Epoch time = 19.242s
[2023-03-23 19:22:39] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:22:39] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:22:40] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:22:40] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:22:40] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:22:40] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:22:40] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:22:40] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:22:41] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:22:41] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:22:41] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:22:41] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:22:41] - INFO: Accuracy on val 0.593
[2023-03-23 19:22:42] - INFO: Epoch: 2, Batch[0/22], Train loss :0.774, Train acc: 0.812
[2023-03-23 19:22:51] - INFO: Epoch: 2, Batch[10/22], Train loss :0.496, Train acc: 0.938
[2023-03-23 19:23:00] - INFO: Epoch: 2, Batch[20/22], Train loss :0.652, Train acc: 0.906
[2023-03-23 19:23:01] - INFO: Epoch: 2, Train loss: 0.624, Epoch time = 19.339s
[2023-03-23 19:23:02] - INFO: Epoch: 3, Batch[0/22], Train loss :0.456, Train acc: 0.906
[2023-03-23 19:23:11] - INFO: Epoch: 3, Batch[10/22], Train loss :0.533, Train acc: 0.906
[2023-03-23 19:23:20] - INFO: Epoch: 3, Batch[20/22], Train loss :0.308, Train acc: 0.969
[2023-03-23 19:23:20] - INFO: Epoch: 3, Train loss: 0.579, Epoch time = 19.384s
[2023-03-23 19:23:20] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:23:20] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:23:21] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:23:21] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:23:21] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:23:21] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:23:21] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:23:21] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:23:22] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:23:22] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:23:22] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:23:22] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:23:22] - INFO: Accuracy on val 0.657
[2023-03-23 19:23:23] - INFO: Epoch: 4, Batch[0/22], Train loss :0.688, Train acc: 0.844
[2023-03-23 19:23:32] - INFO: Epoch: 4, Batch[10/22], Train loss :0.760, Train acc: 0.875
[2023-03-23 19:23:41] - INFO: Epoch: 4, Batch[20/22], Train loss :0.466, Train acc: 0.938
[2023-03-23 19:23:42] - INFO: Epoch: 4, Train loss: 0.533, Epoch time = 19.442s
[2023-03-23 19:23:43] - INFO: Epoch: 5, Batch[0/22], Train loss :0.371, Train acc: 0.938
[2023-03-23 19:23:52] - INFO: Epoch: 5, Batch[10/22], Train loss :0.700, Train acc: 0.875
[2023-03-23 19:24:01] - INFO: Epoch: 5, Batch[20/22], Train loss :0.333, Train acc: 0.938
[2023-03-23 19:24:01] - INFO: Epoch: 5, Train loss: 0.489, Epoch time = 19.377s
[2023-03-23 19:24:01] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:24:01] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:24:02] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:24:02] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:24:02] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:24:02] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:24:02] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:24:02] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:24:03] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:24:03] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:24:03] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:24:03] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:24:03] - INFO: Accuracy on val 0.674
[2023-03-23 19:24:04] - INFO: Epoch: 6, Batch[0/22], Train loss :0.456, Train acc: 0.906
[2023-03-23 19:24:13] - INFO: Epoch: 6, Batch[10/22], Train loss :0.503, Train acc: 0.906
[2023-03-23 19:24:22] - INFO: Epoch: 6, Batch[20/22], Train loss :0.357, Train acc: 0.938
[2023-03-23 19:24:23] - INFO: Epoch: 6, Train loss: 0.446, Epoch time = 19.378s
[2023-03-23 19:24:23] - INFO: Epoch: 7, Batch[0/22], Train loss :0.636, Train acc: 0.875
[2023-03-23 19:24:32] - INFO: Epoch: 7, Batch[10/22], Train loss :0.288, Train acc: 0.938
[2023-03-23 19:24:41] - INFO: Epoch: 7, Batch[20/22], Train loss :0.421, Train acc: 0.938
[2023-03-23 19:24:42] - INFO: Epoch: 7, Train loss: 0.418, Epoch time = 19.394s
[2023-03-23 19:24:42] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:24:42] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:24:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:24:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:24:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:24:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:24:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:24:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:24:43] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:24:43] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:24:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:24:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:24:44] - INFO: Accuracy on val 0.669
[2023-03-23 19:24:45] - INFO: Epoch: 8, Batch[0/22], Train loss :0.271, Train acc: 0.969
[2023-03-23 19:24:54] - INFO: Epoch: 8, Batch[10/22], Train loss :0.527, Train acc: 0.906
[2023-03-23 19:25:03] - INFO: Epoch: 8, Batch[20/22], Train loss :0.190, Train acc: 0.969
[2023-03-23 19:25:03] - INFO: Epoch: 8, Train loss: 0.389, Epoch time = 19.397s
[2023-03-23 19:25:04] - INFO: Epoch: 9, Batch[0/22], Train loss :0.208, Train acc: 1.000
[2023-03-23 19:25:13] - INFO: Epoch: 9, Batch[10/22], Train loss :0.297, Train acc: 0.969
[2023-03-23 19:25:22] - INFO: Epoch: 9, Batch[20/22], Train loss :0.673, Train acc: 0.875
[2023-03-23 19:25:22] - INFO: Epoch: 9, Train loss: 0.360, Epoch time = 19.380s
[2023-03-23 19:25:23] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:25:23] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:25:23] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:25:23] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:25:23] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:25:23] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:25:24] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:25:24] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:25:24] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:25:24] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:25:24] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:25:24] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:25:24] - INFO: Accuracy on val 0.645
[2023-03-23 19:25:25] - INFO: Epoch: 10, Batch[0/22], Train loss :0.434, Train acc: 0.938
[2023-03-23 19:25:34] - INFO: Epoch: 10, Batch[10/22], Train loss :0.257, Train acc: 0.938
[2023-03-23 19:25:43] - INFO: Epoch: 10, Batch[20/22], Train loss :0.150, Train acc: 1.000
[2023-03-23 19:25:43] - INFO: Epoch: 10, Train loss: 0.337, Epoch time = 19.394s
[2023-03-23 19:25:44] - INFO: Epoch: 11, Batch[0/22], Train loss :0.070, Train acc: 1.000
[2023-03-23 19:25:53] - INFO: Epoch: 11, Batch[10/22], Train loss :0.078, Train acc: 1.000
[2023-03-23 19:26:02] - INFO: Epoch: 11, Batch[20/22], Train loss :0.251, Train acc: 0.969
[2023-03-23 19:26:03] - INFO: Epoch: 11, Train loss: 0.332, Epoch time = 19.374s
[2023-03-23 19:26:03] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:26:03] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:26:03] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:26:03] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:26:04] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:26:04] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:26:04] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:26:04] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:26:04] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:26:04] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:26:04] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:26:04] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:26:04] - INFO: Accuracy on val 0.640
[2023-03-23 19:26:05] - INFO: Epoch: 12, Batch[0/22], Train loss :0.325, Train acc: 0.969
[2023-03-23 19:26:14] - INFO: Epoch: 12, Batch[10/22], Train loss :0.200, Train acc: 0.969
[2023-03-23 19:26:23] - INFO: Epoch: 12, Batch[20/22], Train loss :0.155, Train acc: 0.969
[2023-03-23 19:26:24] - INFO: Epoch: 12, Train loss: 0.305, Epoch time = 19.372s
[2023-03-23 19:26:25] - INFO: Epoch: 13, Batch[0/22], Train loss :0.317, Train acc: 0.938
[2023-03-23 19:26:34] - INFO: Epoch: 13, Batch[10/22], Train loss :0.072, Train acc: 1.000
[2023-03-23 19:26:43] - INFO: Epoch: 13, Batch[20/22], Train loss :0.160, Train acc: 0.969
[2023-03-23 19:26:43] - INFO: Epoch: 13, Train loss: 0.284, Epoch time = 19.377s
[2023-03-23 19:26:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:26:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:26:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:26:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:26:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:26:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:26:44] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:26:44] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:26:45] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:26:45] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:26:45] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:26:45] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:26:45] - INFO: Accuracy on val 0.640
[2023-03-23 19:26:46] - INFO: Epoch: 14, Batch[0/22], Train loss :0.383, Train acc: 0.906
[2023-03-23 19:26:55] - INFO: Epoch: 14, Batch[10/22], Train loss :0.641, Train acc: 0.844
[2023-03-23 19:27:04] - INFO: Epoch: 14, Batch[20/22], Train loss :0.345, Train acc: 0.938
[2023-03-23 19:27:04] - INFO: Epoch: 14, Train loss: 0.270, Epoch time = 19.365s
[2023-03-23 19:27:05] - INFO: Epoch: 15, Batch[0/22], Train loss :0.224, Train acc: 0.969
[2023-03-23 19:27:14] - INFO: Epoch: 15, Batch[10/22], Train loss :0.202, Train acc: 0.969
[2023-03-23 19:27:23] - INFO: Epoch: 15, Batch[20/22], Train loss :0.235, Train acc: 0.969
[2023-03-23 19:27:24] - INFO: Epoch: 15, Train loss: 0.261, Epoch time = 19.352s
[2023-03-23 19:27:24] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:27:24] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:27:24] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:27:24] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:27:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:27:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:27:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:27:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:27:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:27:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:27:25] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:27:25] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:27:25] - INFO: Accuracy on val 0.640
[2023-03-23 19:27:27] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 19:27:27] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 19:27:27] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/test_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:27:27] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/train_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:27:27] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/val_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:27:28] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:27:28] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:27:28] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:27:28] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:27:28] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:27:28] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:27:28] - INFO: Acc on test:0.767
[2023-03-23 19:32:09] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 19:32:09] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 19:32:09] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 19:32:09] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 19:32:09] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 19:32:09] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 19:32:09] - INFO: ###  device = cuda:0
[2023-03-23 19:32:09] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 19:32:09] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 19:32:09] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 19:32:09] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 19:32:09] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 19:32:09] - INFO: ###  split_sep = _!_
[2023-03-23 19:32:09] - INFO: ###  is_sample_shuffle = True
[2023-03-23 19:32:09] - INFO: ###  batch_size = 32
[2023-03-23 19:32:09] - INFO: ###  max_sen_len = None
[2023-03-23 19:32:09] - INFO: ###  num_labels = 8191
[2023-03-23 19:32:09] - INFO: ###  epochs = 16
[2023-03-23 19:32:09] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 19:32:09] - INFO: ###  max_position_embeddings = 512
[2023-03-23 19:32:09] - INFO: ###  vocab_size = 21128
[2023-03-23 19:32:09] - INFO: ###  hidden_size = 768
[2023-03-23 19:32:09] - INFO: ###  num_hidden_layers = 12
[2023-03-23 19:32:09] - INFO: ###  num_attention_heads = 12
[2023-03-23 19:32:09] - INFO: ###  hidden_act = gelu
[2023-03-23 19:32:09] - INFO: ###  intermediate_size = 3072
[2023-03-23 19:32:09] - INFO: ###  pad_token_id = 0
[2023-03-23 19:32:09] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 19:32:09] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 19:32:09] - INFO: ###  type_vocab_size = 2
[2023-03-23 19:32:09] - INFO: ###  initializer_range = 0.02
[2023-03-23 19:32:09] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 19:32:09] - INFO: ###  directionality = bidi
[2023-03-23 19:32:09] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 19:32:09] - INFO: ###  model_type = bert
[2023-03-23 19:32:09] - INFO: ###  pooler_fc_size = 768
[2023-03-23 19:32:09] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 19:32:09] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 19:32:09] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 19:32:09] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 19:32:12] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 19:32:13] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 19:32:14] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/test_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:32:14] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/train_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:32:14] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/val_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:32:15] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:32:15] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:32:15] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:32:15] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:32:15] - INFO: y_type:torch.cuda.LongTensor
[2023-03-23 19:32:15] - INFO: logistic_type:torch.cuda.LongTensor
[2023-03-23 19:32:15] - INFO: Acc on test:0.767
[2023-03-23 19:47:29] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 19:47:29] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 19:47:29] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 19:47:29] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 19:47:29] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 19:47:29] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 19:47:29] - INFO: ###  device = cuda:0
[2023-03-23 19:47:29] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 19:47:29] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 19:47:29] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 19:47:29] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 19:47:29] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 19:47:29] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 19:47:29] - INFO: ###  split_sep = _!_
[2023-03-23 19:47:29] - INFO: ###  is_sample_shuffle = True
[2023-03-23 19:47:29] - INFO: ###  batch_size = 32
[2023-03-23 19:47:29] - INFO: ###  max_sen_len = None
[2023-03-23 19:47:29] - INFO: ###  num_labels = 8191
[2023-03-23 19:47:29] - INFO: ###  epochs = 10
[2023-03-23 19:47:29] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 19:47:29] - INFO: ###  max_position_embeddings = 512
[2023-03-23 19:47:29] - INFO: ###  vocab_size = 21128
[2023-03-23 19:47:29] - INFO: ###  hidden_size = 768
[2023-03-23 19:47:29] - INFO: ###  num_hidden_layers = 12
[2023-03-23 19:47:29] - INFO: ###  num_attention_heads = 12
[2023-03-23 19:47:29] - INFO: ###  hidden_act = gelu
[2023-03-23 19:47:29] - INFO: ###  intermediate_size = 3072
[2023-03-23 19:47:29] - INFO: ###  pad_token_id = 0
[2023-03-23 19:47:29] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 19:47:29] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 19:47:29] - INFO: ###  type_vocab_size = 2
[2023-03-23 19:47:29] - INFO: ###  initializer_range = 0.02
[2023-03-23 19:47:29] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 19:47:29] - INFO: ###  directionality = bidi
[2023-03-23 19:47:29] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 19:47:29] - INFO: ###  model_type = bert
[2023-03-23 19:47:29] - INFO: ###  pooler_fc_size = 768
[2023-03-23 19:47:29] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 19:47:29] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 19:47:29] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 19:47:29] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 19:47:31] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 19:47:33] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 19:53:19] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 19:53:19] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 19:53:19] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 19:53:19] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 19:53:19] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 19:53:19] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 19:53:19] - INFO: ###  device = cuda:0
[2023-03-23 19:53:19] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 19:53:19] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 19:53:19] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 19:53:19] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 19:53:19] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 19:53:19] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 19:53:19] - INFO: ###  split_sep = _!_
[2023-03-23 19:53:19] - INFO: ###  is_sample_shuffle = True
[2023-03-23 19:53:19] - INFO: ###  batch_size = 32
[2023-03-23 19:53:19] - INFO: ###  max_sen_len = None
[2023-03-23 19:53:19] - INFO: ###  num_labels = 8191
[2023-03-23 19:53:19] - INFO: ###  epochs = 10
[2023-03-23 19:53:19] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 19:53:19] - INFO: ###  max_position_embeddings = 512
[2023-03-23 19:53:19] - INFO: ###  vocab_size = 21128
[2023-03-23 19:53:19] - INFO: ###  hidden_size = 768
[2023-03-23 19:53:19] - INFO: ###  num_hidden_layers = 12
[2023-03-23 19:53:19] - INFO: ###  num_attention_heads = 12
[2023-03-23 19:53:19] - INFO: ###  hidden_act = gelu
[2023-03-23 19:53:19] - INFO: ###  intermediate_size = 3072
[2023-03-23 19:53:19] - INFO: ###  pad_token_id = 0
[2023-03-23 19:53:19] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 19:53:19] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 19:53:19] - INFO: ###  type_vocab_size = 2
[2023-03-23 19:53:19] - INFO: ###  initializer_range = 0.02
[2023-03-23 19:53:19] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 19:53:19] - INFO: ###  directionality = bidi
[2023-03-23 19:53:19] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 19:53:19] - INFO: ###  model_type = bert
[2023-03-23 19:53:19] - INFO: ###  pooler_fc_size = 768
[2023-03-23 19:53:19] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 19:53:19] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 19:53:19] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 19:53:19] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 19:53:21] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 19:53:23] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 19:53:23] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/test_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:53:23] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/train_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:53:23] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/val_None.pt 存在，直接载入缓存文件！
[2023-03-23 19:53:25] - INFO: Acc on test:0.767
[2023-03-23 19:53:51] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 19:53:51] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 19:53:51] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 19:53:51] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 19:53:51] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 19:53:51] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 19:53:51] - INFO: ###  device = cuda:0
[2023-03-23 19:53:51] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 19:53:51] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 19:53:51] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 19:53:51] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 19:53:51] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 19:53:51] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 19:53:51] - INFO: ###  split_sep = _!_
[2023-03-23 19:53:51] - INFO: ###  is_sample_shuffle = True
[2023-03-23 19:53:51] - INFO: ###  batch_size = 32
[2023-03-23 19:53:51] - INFO: ###  max_sen_len = None
[2023-03-23 19:53:51] - INFO: ###  num_labels = 8191
[2023-03-23 19:53:51] - INFO: ###  epochs = 10
[2023-03-23 19:53:51] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 19:53:51] - INFO: ###  max_position_embeddings = 512
[2023-03-23 19:53:51] - INFO: ###  vocab_size = 21128
[2023-03-23 19:53:51] - INFO: ###  hidden_size = 768
[2023-03-23 19:53:51] - INFO: ###  num_hidden_layers = 12
[2023-03-23 19:53:51] - INFO: ###  num_attention_heads = 12
[2023-03-23 19:53:51] - INFO: ###  hidden_act = gelu
[2023-03-23 19:53:51] - INFO: ###  intermediate_size = 3072
[2023-03-23 19:53:51] - INFO: ###  pad_token_id = 0
[2023-03-23 19:53:51] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 19:53:51] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 19:53:51] - INFO: ###  type_vocab_size = 2
[2023-03-23 19:53:51] - INFO: ###  initializer_range = 0.02
[2023-03-23 19:53:51] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 19:53:51] - INFO: ###  directionality = bidi
[2023-03-23 19:53:51] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 19:53:51] - INFO: ###  model_type = bert
[2023-03-23 19:53:51] - INFO: ###  pooler_fc_size = 768
[2023-03-23 19:53:51] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 19:53:51] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 19:53:51] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 19:53:51] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 19:53:53] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 19:53:55] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 19:55:25] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 19:55:25] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 19:55:25] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 19:55:25] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 19:55:25] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 19:55:25] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 19:55:25] - INFO: ###  device = cuda:0
[2023-03-23 19:55:25] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 19:55:25] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 19:55:25] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 19:55:25] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 19:55:25] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 19:55:25] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 19:55:25] - INFO: ###  split_sep = _!_
[2023-03-23 19:55:25] - INFO: ###  is_sample_shuffle = True
[2023-03-23 19:55:25] - INFO: ###  batch_size = 32
[2023-03-23 19:55:25] - INFO: ###  max_sen_len = None
[2023-03-23 19:55:25] - INFO: ###  num_labels = 8191
[2023-03-23 19:55:25] - INFO: ###  epochs = 10
[2023-03-23 19:55:25] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 19:55:25] - INFO: ###  max_position_embeddings = 512
[2023-03-23 19:55:25] - INFO: ###  vocab_size = 21128
[2023-03-23 19:55:25] - INFO: ###  hidden_size = 768
[2023-03-23 19:55:25] - INFO: ###  num_hidden_layers = 12
[2023-03-23 19:55:25] - INFO: ###  num_attention_heads = 12
[2023-03-23 19:55:25] - INFO: ###  hidden_act = gelu
[2023-03-23 19:55:25] - INFO: ###  intermediate_size = 3072
[2023-03-23 19:55:25] - INFO: ###  pad_token_id = 0
[2023-03-23 19:55:25] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 19:55:25] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 19:55:25] - INFO: ###  type_vocab_size = 2
[2023-03-23 19:55:25] - INFO: ###  initializer_range = 0.02
[2023-03-23 19:55:25] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 19:55:25] - INFO: ###  directionality = bidi
[2023-03-23 19:55:25] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 19:55:25] - INFO: ###  model_type = bert
[2023-03-23 19:55:25] - INFO: ###  pooler_fc_size = 768
[2023-03-23 19:55:25] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 19:55:25] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 19:55:25] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 19:55:25] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 19:55:27] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 19:55:29] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 19:55:30] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/predict_None.pt 不存在，重新处理并缓存！
[2023-03-23 20:02:54] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 20:02:54] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 20:02:54] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 20:02:54] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 20:02:54] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 20:02:54] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 20:02:54] - INFO: ###  device = cuda:0
[2023-03-23 20:02:54] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 20:02:54] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 20:02:54] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 20:02:54] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 20:02:54] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 20:02:54] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 20:02:54] - INFO: ###  split_sep = _!_
[2023-03-23 20:02:54] - INFO: ###  is_sample_shuffle = True
[2023-03-23 20:02:54] - INFO: ###  batch_size = 32
[2023-03-23 20:02:54] - INFO: ###  max_sen_len = None
[2023-03-23 20:02:54] - INFO: ###  num_labels = 8191
[2023-03-23 20:02:54] - INFO: ###  epochs = 10
[2023-03-23 20:02:54] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 20:02:54] - INFO: ###  max_position_embeddings = 512
[2023-03-23 20:02:54] - INFO: ###  vocab_size = 21128
[2023-03-23 20:02:54] - INFO: ###  hidden_size = 768
[2023-03-23 20:02:54] - INFO: ###  num_hidden_layers = 12
[2023-03-23 20:02:54] - INFO: ###  num_attention_heads = 12
[2023-03-23 20:02:54] - INFO: ###  hidden_act = gelu
[2023-03-23 20:02:54] - INFO: ###  intermediate_size = 3072
[2023-03-23 20:02:54] - INFO: ###  pad_token_id = 0
[2023-03-23 20:02:54] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 20:02:54] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 20:02:54] - INFO: ###  type_vocab_size = 2
[2023-03-23 20:02:54] - INFO: ###  initializer_range = 0.02
[2023-03-23 20:02:54] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 20:02:54] - INFO: ###  directionality = bidi
[2023-03-23 20:02:54] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 20:02:54] - INFO: ###  model_type = bert
[2023-03-23 20:02:54] - INFO: ###  pooler_fc_size = 768
[2023-03-23 20:02:54] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 20:02:54] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 20:02:54] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 20:02:54] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 20:02:57] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 20:02:59] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 20:02:59] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/predict_None.pt 不存在，重新处理并缓存！
[2023-03-23 20:04:29] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 20:04:29] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 20:04:29] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 20:04:29] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 20:04:29] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 20:04:29] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 20:04:29] - INFO: ###  device = cuda:0
[2023-03-23 20:04:29] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 20:04:29] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 20:04:29] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 20:04:29] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 20:04:29] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 20:04:29] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 20:04:29] - INFO: ###  split_sep = _!_
[2023-03-23 20:04:29] - INFO: ###  is_sample_shuffle = True
[2023-03-23 20:04:29] - INFO: ###  batch_size = 32
[2023-03-23 20:04:29] - INFO: ###  max_sen_len = None
[2023-03-23 20:04:29] - INFO: ###  num_labels = 8191
[2023-03-23 20:04:29] - INFO: ###  epochs = 10
[2023-03-23 20:04:29] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 20:04:29] - INFO: ###  max_position_embeddings = 512
[2023-03-23 20:04:29] - INFO: ###  vocab_size = 21128
[2023-03-23 20:04:29] - INFO: ###  hidden_size = 768
[2023-03-23 20:04:29] - INFO: ###  num_hidden_layers = 12
[2023-03-23 20:04:29] - INFO: ###  num_attention_heads = 12
[2023-03-23 20:04:29] - INFO: ###  hidden_act = gelu
[2023-03-23 20:04:29] - INFO: ###  intermediate_size = 3072
[2023-03-23 20:04:29] - INFO: ###  pad_token_id = 0
[2023-03-23 20:04:29] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 20:04:29] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 20:04:29] - INFO: ###  type_vocab_size = 2
[2023-03-23 20:04:29] - INFO: ###  initializer_range = 0.02
[2023-03-23 20:04:29] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 20:04:29] - INFO: ###  directionality = bidi
[2023-03-23 20:04:29] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 20:04:29] - INFO: ###  model_type = bert
[2023-03-23 20:04:29] - INFO: ###  pooler_fc_size = 768
[2023-03-23 20:04:29] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 20:04:29] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 20:04:29] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 20:04:29] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 20:04:31] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 20:04:34] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 20:04:34] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/predict_None.pt 存在，直接载入缓存文件！
[2023-03-23 20:05:51] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 20:05:51] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 20:05:51] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 20:05:51] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 20:05:51] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 20:05:51] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 20:05:51] - INFO: ###  device = cuda:0
[2023-03-23 20:05:51] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 20:05:51] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 20:05:51] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 20:05:51] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 20:05:51] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 20:05:51] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 20:05:51] - INFO: ###  split_sep = _!_
[2023-03-23 20:05:51] - INFO: ###  is_sample_shuffle = True
[2023-03-23 20:05:51] - INFO: ###  batch_size = 32
[2023-03-23 20:05:51] - INFO: ###  max_sen_len = None
[2023-03-23 20:05:51] - INFO: ###  num_labels = 8191
[2023-03-23 20:05:51] - INFO: ###  epochs = 10
[2023-03-23 20:05:51] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 20:05:51] - INFO: ###  max_position_embeddings = 512
[2023-03-23 20:05:51] - INFO: ###  vocab_size = 21128
[2023-03-23 20:05:51] - INFO: ###  hidden_size = 768
[2023-03-23 20:05:51] - INFO: ###  num_hidden_layers = 12
[2023-03-23 20:05:51] - INFO: ###  num_attention_heads = 12
[2023-03-23 20:05:51] - INFO: ###  hidden_act = gelu
[2023-03-23 20:05:51] - INFO: ###  intermediate_size = 3072
[2023-03-23 20:05:51] - INFO: ###  pad_token_id = 0
[2023-03-23 20:05:51] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 20:05:51] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 20:05:51] - INFO: ###  type_vocab_size = 2
[2023-03-23 20:05:51] - INFO: ###  initializer_range = 0.02
[2023-03-23 20:05:51] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 20:05:51] - INFO: ###  directionality = bidi
[2023-03-23 20:05:51] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 20:05:51] - INFO: ###  model_type = bert
[2023-03-23 20:05:51] - INFO: ###  pooler_fc_size = 768
[2023-03-23 20:05:51] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 20:05:51] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 20:05:51] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 20:05:51] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 20:05:53] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 20:05:55] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 20:05:56] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/predict_None.pt 存在，直接载入缓存文件！
[2023-03-23 20:06:38] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 20:06:38] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 20:06:38] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 20:06:38] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 20:06:38] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 20:06:38] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 20:06:38] - INFO: ###  device = cuda:0
[2023-03-23 20:06:38] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 20:06:38] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 20:06:38] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 20:06:38] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 20:06:38] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 20:06:38] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 20:06:38] - INFO: ###  split_sep = _!_
[2023-03-23 20:06:38] - INFO: ###  is_sample_shuffle = True
[2023-03-23 20:06:38] - INFO: ###  batch_size = 32
[2023-03-23 20:06:38] - INFO: ###  max_sen_len = None
[2023-03-23 20:06:38] - INFO: ###  num_labels = 8191
[2023-03-23 20:06:38] - INFO: ###  epochs = 10
[2023-03-23 20:06:38] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 20:06:38] - INFO: ###  max_position_embeddings = 512
[2023-03-23 20:06:38] - INFO: ###  vocab_size = 21128
[2023-03-23 20:06:38] - INFO: ###  hidden_size = 768
[2023-03-23 20:06:38] - INFO: ###  num_hidden_layers = 12
[2023-03-23 20:06:38] - INFO: ###  num_attention_heads = 12
[2023-03-23 20:06:38] - INFO: ###  hidden_act = gelu
[2023-03-23 20:06:38] - INFO: ###  intermediate_size = 3072
[2023-03-23 20:06:38] - INFO: ###  pad_token_id = 0
[2023-03-23 20:06:38] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 20:06:38] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 20:06:38] - INFO: ###  type_vocab_size = 2
[2023-03-23 20:06:38] - INFO: ###  initializer_range = 0.02
[2023-03-23 20:06:38] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 20:06:38] - INFO: ###  directionality = bidi
[2023-03-23 20:06:38] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 20:06:38] - INFO: ###  model_type = bert
[2023-03-23 20:06:38] - INFO: ###  pooler_fc_size = 768
[2023-03-23 20:06:38] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 20:06:38] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 20:06:38] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 20:06:38] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 20:06:40] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 20:06:42] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 20:06:42] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/predict_None.pt 存在，直接载入缓存文件！
[2023-03-23 20:09:30] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 20:09:30] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 20:09:30] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 20:09:30] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 20:09:30] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 20:09:30] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 20:09:30] - INFO: ###  device = cuda:0
[2023-03-23 20:09:30] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 20:09:30] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 20:09:30] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 20:09:30] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 20:09:30] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 20:09:30] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 20:09:30] - INFO: ###  split_sep = _!_
[2023-03-23 20:09:30] - INFO: ###  is_sample_shuffle = True
[2023-03-23 20:09:30] - INFO: ###  batch_size = 32
[2023-03-23 20:09:30] - INFO: ###  max_sen_len = None
[2023-03-23 20:09:30] - INFO: ###  num_labels = 8191
[2023-03-23 20:09:30] - INFO: ###  epochs = 10
[2023-03-23 20:09:30] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 20:09:30] - INFO: ###  max_position_embeddings = 512
[2023-03-23 20:09:30] - INFO: ###  vocab_size = 21128
[2023-03-23 20:09:30] - INFO: ###  hidden_size = 768
[2023-03-23 20:09:30] - INFO: ###  num_hidden_layers = 12
[2023-03-23 20:09:30] - INFO: ###  num_attention_heads = 12
[2023-03-23 20:09:30] - INFO: ###  hidden_act = gelu
[2023-03-23 20:09:30] - INFO: ###  intermediate_size = 3072
[2023-03-23 20:09:30] - INFO: ###  pad_token_id = 0
[2023-03-23 20:09:30] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 20:09:30] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 20:09:30] - INFO: ###  type_vocab_size = 2
[2023-03-23 20:09:30] - INFO: ###  initializer_range = 0.02
[2023-03-23 20:09:30] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 20:09:30] - INFO: ###  directionality = bidi
[2023-03-23 20:09:30] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 20:09:30] - INFO: ###  model_type = bert
[2023-03-23 20:09:30] - INFO: ###  pooler_fc_size = 768
[2023-03-23 20:09:30] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 20:09:30] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 20:09:30] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 20:09:30] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 20:09:32] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 20:09:34] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 20:09:34] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/predict_None.pt 存在，直接载入缓存文件！
[2023-03-23 20:12:11] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 20:12:11] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 20:12:11] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 20:12:11] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 20:12:11] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 20:12:11] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 20:12:11] - INFO: ###  device = cuda:0
[2023-03-23 20:12:11] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 20:12:11] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 20:12:11] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 20:12:11] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 20:12:11] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 20:12:11] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 20:12:11] - INFO: ###  split_sep = _!_
[2023-03-23 20:12:11] - INFO: ###  is_sample_shuffle = True
[2023-03-23 20:12:11] - INFO: ###  batch_size = 32
[2023-03-23 20:12:11] - INFO: ###  max_sen_len = None
[2023-03-23 20:12:11] - INFO: ###  num_labels = 8191
[2023-03-23 20:12:11] - INFO: ###  epochs = 10
[2023-03-23 20:12:11] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 20:12:11] - INFO: ###  max_position_embeddings = 512
[2023-03-23 20:12:11] - INFO: ###  vocab_size = 21128
[2023-03-23 20:12:11] - INFO: ###  hidden_size = 768
[2023-03-23 20:12:11] - INFO: ###  num_hidden_layers = 12
[2023-03-23 20:12:11] - INFO: ###  num_attention_heads = 12
[2023-03-23 20:12:11] - INFO: ###  hidden_act = gelu
[2023-03-23 20:12:11] - INFO: ###  intermediate_size = 3072
[2023-03-23 20:12:11] - INFO: ###  pad_token_id = 0
[2023-03-23 20:12:11] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 20:12:11] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 20:12:11] - INFO: ###  type_vocab_size = 2
[2023-03-23 20:12:11] - INFO: ###  initializer_range = 0.02
[2023-03-23 20:12:11] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 20:12:11] - INFO: ###  directionality = bidi
[2023-03-23 20:12:11] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 20:12:11] - INFO: ###  model_type = bert
[2023-03-23 20:12:11] - INFO: ###  pooler_fc_size = 768
[2023-03-23 20:12:11] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 20:12:11] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 20:12:11] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 20:12:11] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 20:12:13] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 20:12:15] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 20:12:16] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/predict_None.pt 存在，直接载入缓存文件！
[2023-03-23 20:14:12] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 20:14:12] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 20:14:12] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 20:14:12] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 20:14:12] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 20:14:12] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 20:14:12] - INFO: ###  device = cuda:0
[2023-03-23 20:14:12] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 20:14:12] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 20:14:12] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 20:14:12] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 20:14:12] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 20:14:12] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 20:14:12] - INFO: ###  split_sep = _!_
[2023-03-23 20:14:12] - INFO: ###  is_sample_shuffle = True
[2023-03-23 20:14:12] - INFO: ###  batch_size = 32
[2023-03-23 20:14:12] - INFO: ###  max_sen_len = None
[2023-03-23 20:14:12] - INFO: ###  num_labels = 8191
[2023-03-23 20:14:12] - INFO: ###  epochs = 10
[2023-03-23 20:14:12] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 20:14:12] - INFO: ###  max_position_embeddings = 512
[2023-03-23 20:14:12] - INFO: ###  vocab_size = 21128
[2023-03-23 20:14:12] - INFO: ###  hidden_size = 768
[2023-03-23 20:14:12] - INFO: ###  num_hidden_layers = 12
[2023-03-23 20:14:12] - INFO: ###  num_attention_heads = 12
[2023-03-23 20:14:12] - INFO: ###  hidden_act = gelu
[2023-03-23 20:14:12] - INFO: ###  intermediate_size = 3072
[2023-03-23 20:14:12] - INFO: ###  pad_token_id = 0
[2023-03-23 20:14:12] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 20:14:12] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 20:14:12] - INFO: ###  type_vocab_size = 2
[2023-03-23 20:14:12] - INFO: ###  initializer_range = 0.02
[2023-03-23 20:14:12] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 20:14:12] - INFO: ###  directionality = bidi
[2023-03-23 20:14:12] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 20:14:12] - INFO: ###  model_type = bert
[2023-03-23 20:14:12] - INFO: ###  pooler_fc_size = 768
[2023-03-23 20:14:12] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 20:14:12] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 20:14:12] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 20:14:12] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 20:14:14] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 20:14:16] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 20:14:16] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/test_None.pt 不存在，重新处理并缓存！
[2023-03-23 20:14:17] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/train_None.pt 不存在，重新处理并缓存！
[2023-03-23 20:14:19] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/val_None.pt 不存在，重新处理并缓存！
[2023-03-23 20:14:21] - INFO: Acc on test:0.767
[2023-03-23 20:19:59] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 20:19:59] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 20:19:59] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 20:19:59] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 20:19:59] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 20:19:59] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 20:19:59] - INFO: ###  device = cuda:0
[2023-03-23 20:19:59] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 20:19:59] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 20:19:59] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 20:19:59] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 20:19:59] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 20:19:59] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 20:19:59] - INFO: ###  split_sep = _!_
[2023-03-23 20:19:59] - INFO: ###  is_sample_shuffle = True
[2023-03-23 20:19:59] - INFO: ###  batch_size = 32
[2023-03-23 20:19:59] - INFO: ###  max_sen_len = None
[2023-03-23 20:19:59] - INFO: ###  num_labels = 8191
[2023-03-23 20:19:59] - INFO: ###  epochs = 10
[2023-03-23 20:19:59] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 20:19:59] - INFO: ###  max_position_embeddings = 512
[2023-03-23 20:19:59] - INFO: ###  vocab_size = 21128
[2023-03-23 20:19:59] - INFO: ###  hidden_size = 768
[2023-03-23 20:19:59] - INFO: ###  num_hidden_layers = 12
[2023-03-23 20:19:59] - INFO: ###  num_attention_heads = 12
[2023-03-23 20:19:59] - INFO: ###  hidden_act = gelu
[2023-03-23 20:19:59] - INFO: ###  intermediate_size = 3072
[2023-03-23 20:19:59] - INFO: ###  pad_token_id = 0
[2023-03-23 20:19:59] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 20:19:59] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 20:19:59] - INFO: ###  type_vocab_size = 2
[2023-03-23 20:19:59] - INFO: ###  initializer_range = 0.02
[2023-03-23 20:19:59] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 20:19:59] - INFO: ###  directionality = bidi
[2023-03-23 20:19:59] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 20:19:59] - INFO: ###  model_type = bert
[2023-03-23 20:19:59] - INFO: ###  pooler_fc_size = 768
[2023-03-23 20:19:59] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 20:19:59] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 20:19:59] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 20:19:59] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 20:20:01] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 20:20:03] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 20:20:03] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/predict_None.pt 存在，直接载入缓存文件！
[2023-03-23 20:21:29] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 20:21:29] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 20:21:29] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 20:21:29] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 20:21:29] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 20:21:29] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 20:21:29] - INFO: ###  device = cuda:0
[2023-03-23 20:21:29] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 20:21:29] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 20:21:29] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 20:21:29] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 20:21:29] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 20:21:29] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 20:21:29] - INFO: ###  split_sep = _!_
[2023-03-23 20:21:29] - INFO: ###  is_sample_shuffle = True
[2023-03-23 20:21:29] - INFO: ###  batch_size = 32
[2023-03-23 20:21:29] - INFO: ###  max_sen_len = None
[2023-03-23 20:21:29] - INFO: ###  num_labels = 8191
[2023-03-23 20:21:29] - INFO: ###  epochs = 10
[2023-03-23 20:21:29] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 20:21:29] - INFO: ###  max_position_embeddings = 512
[2023-03-23 20:21:29] - INFO: ###  vocab_size = 21128
[2023-03-23 20:21:29] - INFO: ###  hidden_size = 768
[2023-03-23 20:21:29] - INFO: ###  num_hidden_layers = 12
[2023-03-23 20:21:29] - INFO: ###  num_attention_heads = 12
[2023-03-23 20:21:29] - INFO: ###  hidden_act = gelu
[2023-03-23 20:21:29] - INFO: ###  intermediate_size = 3072
[2023-03-23 20:21:29] - INFO: ###  pad_token_id = 0
[2023-03-23 20:21:29] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 20:21:29] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 20:21:29] - INFO: ###  type_vocab_size = 2
[2023-03-23 20:21:29] - INFO: ###  initializer_range = 0.02
[2023-03-23 20:21:29] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 20:21:29] - INFO: ###  directionality = bidi
[2023-03-23 20:21:29] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 20:21:29] - INFO: ###  model_type = bert
[2023-03-23 20:21:29] - INFO: ###  pooler_fc_size = 768
[2023-03-23 20:21:29] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 20:21:29] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 20:21:29] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 20:21:29] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 20:21:31] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 20:21:33] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 20:21:34] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/predict_None.pt 存在，直接载入缓存文件！
[2023-03-23 20:23:39] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 20:23:39] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 20:23:39] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 20:23:39] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 20:23:39] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 20:23:39] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 20:23:39] - INFO: ###  device = cuda:0
[2023-03-23 20:23:39] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 20:23:39] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 20:23:39] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 20:23:39] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 20:23:39] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 20:23:39] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 20:23:39] - INFO: ###  split_sep = _!_
[2023-03-23 20:23:39] - INFO: ###  is_sample_shuffle = True
[2023-03-23 20:23:39] - INFO: ###  batch_size = 32
[2023-03-23 20:23:39] - INFO: ###  max_sen_len = None
[2023-03-23 20:23:39] - INFO: ###  num_labels = 8191
[2023-03-23 20:23:39] - INFO: ###  epochs = 10
[2023-03-23 20:23:39] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 20:23:39] - INFO: ###  max_position_embeddings = 512
[2023-03-23 20:23:39] - INFO: ###  vocab_size = 21128
[2023-03-23 20:23:39] - INFO: ###  hidden_size = 768
[2023-03-23 20:23:39] - INFO: ###  num_hidden_layers = 12
[2023-03-23 20:23:39] - INFO: ###  num_attention_heads = 12
[2023-03-23 20:23:39] - INFO: ###  hidden_act = gelu
[2023-03-23 20:23:39] - INFO: ###  intermediate_size = 3072
[2023-03-23 20:23:39] - INFO: ###  pad_token_id = 0
[2023-03-23 20:23:39] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 20:23:39] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 20:23:39] - INFO: ###  type_vocab_size = 2
[2023-03-23 20:23:39] - INFO: ###  initializer_range = 0.02
[2023-03-23 20:23:39] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 20:23:39] - INFO: ###  directionality = bidi
[2023-03-23 20:23:39] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 20:23:39] - INFO: ###  model_type = bert
[2023-03-23 20:23:39] - INFO: ###  pooler_fc_size = 768
[2023-03-23 20:23:39] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 20:23:39] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 20:23:39] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 20:23:39] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 20:23:41] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 20:23:44] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 20:23:44] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/predict_None.pt 不存在，重新处理并缓存！
[2023-03-23 20:27:25] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 20:27:25] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 20:27:25] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 20:27:25] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 20:27:25] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 20:27:25] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 20:27:25] - INFO: ###  device = cuda:0
[2023-03-23 20:27:25] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 20:27:25] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 20:27:25] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 20:27:25] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 20:27:25] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 20:27:25] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 20:27:25] - INFO: ###  split_sep = _!_
[2023-03-23 20:27:25] - INFO: ###  is_sample_shuffle = True
[2023-03-23 20:27:25] - INFO: ###  batch_size = 32
[2023-03-23 20:27:25] - INFO: ###  max_sen_len = None
[2023-03-23 20:27:25] - INFO: ###  num_labels = 8191
[2023-03-23 20:27:25] - INFO: ###  epochs = 10
[2023-03-23 20:27:25] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 20:27:25] - INFO: ###  max_position_embeddings = 512
[2023-03-23 20:27:25] - INFO: ###  vocab_size = 21128
[2023-03-23 20:27:25] - INFO: ###  hidden_size = 768
[2023-03-23 20:27:25] - INFO: ###  num_hidden_layers = 12
[2023-03-23 20:27:25] - INFO: ###  num_attention_heads = 12
[2023-03-23 20:27:25] - INFO: ###  hidden_act = gelu
[2023-03-23 20:27:25] - INFO: ###  intermediate_size = 3072
[2023-03-23 20:27:25] - INFO: ###  pad_token_id = 0
[2023-03-23 20:27:25] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 20:27:25] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 20:27:25] - INFO: ###  type_vocab_size = 2
[2023-03-23 20:27:25] - INFO: ###  initializer_range = 0.02
[2023-03-23 20:27:25] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 20:27:25] - INFO: ###  directionality = bidi
[2023-03-23 20:27:25] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 20:27:25] - INFO: ###  model_type = bert
[2023-03-23 20:27:25] - INFO: ###  pooler_fc_size = 768
[2023-03-23 20:27:25] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 20:27:25] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 20:27:25] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 20:27:25] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 20:27:27] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 20:27:29] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 20:27:29] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/predict_None.pt 存在，直接载入缓存文件！
[2023-03-23 20:29:18] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 20:29:18] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 20:29:18] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 20:29:18] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 20:29:18] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 20:29:18] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 20:29:18] - INFO: ###  device = cuda:0
[2023-03-23 20:29:18] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 20:29:18] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 20:29:18] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 20:29:18] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 20:29:18] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 20:29:18] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 20:29:18] - INFO: ###  split_sep = _!_
[2023-03-23 20:29:18] - INFO: ###  is_sample_shuffle = True
[2023-03-23 20:29:18] - INFO: ###  batch_size = 32
[2023-03-23 20:29:18] - INFO: ###  max_sen_len = None
[2023-03-23 20:29:18] - INFO: ###  num_labels = 8191
[2023-03-23 20:29:18] - INFO: ###  epochs = 10
[2023-03-23 20:29:18] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 20:29:18] - INFO: ###  max_position_embeddings = 512
[2023-03-23 20:29:18] - INFO: ###  vocab_size = 21128
[2023-03-23 20:29:18] - INFO: ###  hidden_size = 768
[2023-03-23 20:29:18] - INFO: ###  num_hidden_layers = 12
[2023-03-23 20:29:18] - INFO: ###  num_attention_heads = 12
[2023-03-23 20:29:18] - INFO: ###  hidden_act = gelu
[2023-03-23 20:29:18] - INFO: ###  intermediate_size = 3072
[2023-03-23 20:29:18] - INFO: ###  pad_token_id = 0
[2023-03-23 20:29:18] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 20:29:18] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 20:29:18] - INFO: ###  type_vocab_size = 2
[2023-03-23 20:29:18] - INFO: ###  initializer_range = 0.02
[2023-03-23 20:29:18] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 20:29:18] - INFO: ###  directionality = bidi
[2023-03-23 20:29:18] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 20:29:18] - INFO: ###  model_type = bert
[2023-03-23 20:29:18] - INFO: ###  pooler_fc_size = 768
[2023-03-23 20:29:18] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 20:29:18] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 20:29:18] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 20:29:18] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 20:31:29] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/predict_cache.pt 不存在，重新处理并缓存！
[2023-03-23 20:32:00] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/predict_cache.pt 存在，直接载入缓存文件！
[2023-03-23 20:50:42] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 20:50:42] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 20:50:42] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 20:50:42] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 20:50:42] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 20:50:42] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 20:50:42] - INFO: ###  device = cuda:0
[2023-03-23 20:50:42] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 20:50:42] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 20:50:42] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 20:50:42] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 20:50:42] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 20:50:42] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 20:50:42] - INFO: ###  split_sep = _!_
[2023-03-23 20:50:42] - INFO: ###  is_sample_shuffle = True
[2023-03-23 20:50:42] - INFO: ###  batch_size = 32
[2023-03-23 20:50:42] - INFO: ###  max_sen_len = None
[2023-03-23 20:50:42] - INFO: ###  num_labels = 8191
[2023-03-23 20:50:42] - INFO: ###  epochs = 10
[2023-03-23 20:50:42] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 20:50:42] - INFO: ###  max_position_embeddings = 512
[2023-03-23 20:50:42] - INFO: ###  vocab_size = 21128
[2023-03-23 20:50:42] - INFO: ###  hidden_size = 768
[2023-03-23 20:50:42] - INFO: ###  num_hidden_layers = 12
[2023-03-23 20:50:42] - INFO: ###  num_attention_heads = 12
[2023-03-23 20:50:42] - INFO: ###  hidden_act = gelu
[2023-03-23 20:50:42] - INFO: ###  intermediate_size = 3072
[2023-03-23 20:50:42] - INFO: ###  pad_token_id = 0
[2023-03-23 20:50:42] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 20:50:42] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 20:50:42] - INFO: ###  type_vocab_size = 2
[2023-03-23 20:50:42] - INFO: ###  initializer_range = 0.02
[2023-03-23 20:50:42] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 20:50:42] - INFO: ###  directionality = bidi
[2023-03-23 20:50:42] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 20:50:42] - INFO: ###  model_type = bert
[2023-03-23 20:50:42] - INFO: ###  pooler_fc_size = 768
[2023-03-23 20:50:42] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 20:50:42] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 20:50:42] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 20:50:42] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 20:50:44] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 20:50:46] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 20:50:46] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/predict_None.pt 存在，直接载入缓存文件！
[2023-03-23 20:51:53] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 20:51:53] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 20:51:53] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 20:51:53] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 20:51:53] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 20:51:53] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 20:51:53] - INFO: ###  device = cuda:0
[2023-03-23 20:51:53] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 20:51:53] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 20:51:53] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 20:51:53] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 20:51:53] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 20:51:53] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 20:51:53] - INFO: ###  split_sep = _!_
[2023-03-23 20:51:53] - INFO: ###  is_sample_shuffle = True
[2023-03-23 20:51:53] - INFO: ###  batch_size = 32
[2023-03-23 20:51:53] - INFO: ###  max_sen_len = None
[2023-03-23 20:51:53] - INFO: ###  num_labels = 8191
[2023-03-23 20:51:53] - INFO: ###  epochs = 10
[2023-03-23 20:51:53] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 20:51:53] - INFO: ###  max_position_embeddings = 512
[2023-03-23 20:51:53] - INFO: ###  vocab_size = 21128
[2023-03-23 20:51:53] - INFO: ###  hidden_size = 768
[2023-03-23 20:51:53] - INFO: ###  num_hidden_layers = 12
[2023-03-23 20:51:53] - INFO: ###  num_attention_heads = 12
[2023-03-23 20:51:53] - INFO: ###  hidden_act = gelu
[2023-03-23 20:51:53] - INFO: ###  intermediate_size = 3072
[2023-03-23 20:51:53] - INFO: ###  pad_token_id = 0
[2023-03-23 20:51:53] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 20:51:53] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 20:51:53] - INFO: ###  type_vocab_size = 2
[2023-03-23 20:51:53] - INFO: ###  initializer_range = 0.02
[2023-03-23 20:51:53] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 20:51:53] - INFO: ###  directionality = bidi
[2023-03-23 20:51:53] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 20:51:53] - INFO: ###  model_type = bert
[2023-03-23 20:51:53] - INFO: ###  pooler_fc_size = 768
[2023-03-23 20:51:53] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 20:51:53] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 20:51:53] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 20:51:53] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 20:51:55] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 20:51:57] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 20:51:57] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/predict_None.pt 存在，直接载入缓存文件！
[2023-03-23 20:53:42] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 20:53:42] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 20:53:42] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 20:53:42] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 20:53:42] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 20:53:42] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 20:53:42] - INFO: ###  device = cuda:0
[2023-03-23 20:53:42] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 20:53:42] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 20:53:42] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 20:53:42] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 20:53:42] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 20:53:42] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 20:53:42] - INFO: ###  split_sep = _!_
[2023-03-23 20:53:42] - INFO: ###  is_sample_shuffle = True
[2023-03-23 20:53:42] - INFO: ###  batch_size = 32
[2023-03-23 20:53:42] - INFO: ###  max_sen_len = None
[2023-03-23 20:53:42] - INFO: ###  num_labels = 8191
[2023-03-23 20:53:42] - INFO: ###  epochs = 10
[2023-03-23 20:53:42] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 20:53:42] - INFO: ###  max_position_embeddings = 512
[2023-03-23 20:53:42] - INFO: ###  vocab_size = 21128
[2023-03-23 20:53:42] - INFO: ###  hidden_size = 768
[2023-03-23 20:53:42] - INFO: ###  num_hidden_layers = 12
[2023-03-23 20:53:42] - INFO: ###  num_attention_heads = 12
[2023-03-23 20:53:42] - INFO: ###  hidden_act = gelu
[2023-03-23 20:53:42] - INFO: ###  intermediate_size = 3072
[2023-03-23 20:53:42] - INFO: ###  pad_token_id = 0
[2023-03-23 20:53:42] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 20:53:42] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 20:53:42] - INFO: ###  type_vocab_size = 2
[2023-03-23 20:53:42] - INFO: ###  initializer_range = 0.02
[2023-03-23 20:53:42] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 20:53:42] - INFO: ###  directionality = bidi
[2023-03-23 20:53:42] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 20:53:42] - INFO: ###  model_type = bert
[2023-03-23 20:53:42] - INFO: ###  pooler_fc_size = 768
[2023-03-23 20:53:42] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 20:53:42] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 20:53:42] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 20:53:42] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 20:53:43] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 20:53:46] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 20:53:46] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/predict_None.pt 存在，直接载入缓存文件！
[2023-03-23 21:05:01] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 21:05:01] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 21:05:01] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 21:05:01] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 21:05:01] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 21:05:01] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 21:05:01] - INFO: ###  device = cuda:0
[2023-03-23 21:05:01] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 21:05:01] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 21:05:01] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 21:05:01] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 21:05:01] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 21:05:01] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 21:05:01] - INFO: ###  split_sep = _!_
[2023-03-23 21:05:01] - INFO: ###  is_sample_shuffle = True
[2023-03-23 21:05:01] - INFO: ###  batch_size = 32
[2023-03-23 21:05:01] - INFO: ###  max_sen_len = None
[2023-03-23 21:05:01] - INFO: ###  num_labels = 8191
[2023-03-23 21:05:01] - INFO: ###  epochs = 10
[2023-03-23 21:05:01] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 21:05:01] - INFO: ###  max_position_embeddings = 512
[2023-03-23 21:05:01] - INFO: ###  vocab_size = 21128
[2023-03-23 21:05:01] - INFO: ###  hidden_size = 768
[2023-03-23 21:05:01] - INFO: ###  num_hidden_layers = 12
[2023-03-23 21:05:01] - INFO: ###  num_attention_heads = 12
[2023-03-23 21:05:01] - INFO: ###  hidden_act = gelu
[2023-03-23 21:05:01] - INFO: ###  intermediate_size = 3072
[2023-03-23 21:05:01] - INFO: ###  pad_token_id = 0
[2023-03-23 21:05:01] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 21:05:01] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 21:05:01] - INFO: ###  type_vocab_size = 2
[2023-03-23 21:05:01] - INFO: ###  initializer_range = 0.02
[2023-03-23 21:05:01] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 21:05:01] - INFO: ###  directionality = bidi
[2023-03-23 21:05:01] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 21:05:01] - INFO: ###  model_type = bert
[2023-03-23 21:05:01] - INFO: ###  pooler_fc_size = 768
[2023-03-23 21:05:01] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 21:05:01] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 21:05:01] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 21:05:01] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 21:05:03] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 21:05:05] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 21:05:05] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/predict_None.pt 存在，直接载入缓存文件！
[2023-03-23 21:07:23] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 21:07:23] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 21:07:23] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 21:07:23] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 21:07:23] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 21:07:23] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 21:07:23] - INFO: ###  device = cuda:0
[2023-03-23 21:07:23] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 21:07:23] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 21:07:23] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 21:07:23] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 21:07:23] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 21:07:23] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 21:07:23] - INFO: ###  split_sep = _!_
[2023-03-23 21:07:23] - INFO: ###  is_sample_shuffle = True
[2023-03-23 21:07:23] - INFO: ###  batch_size = 32
[2023-03-23 21:07:23] - INFO: ###  max_sen_len = None
[2023-03-23 21:07:23] - INFO: ###  num_labels = 8191
[2023-03-23 21:07:23] - INFO: ###  epochs = 10
[2023-03-23 21:07:23] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 21:07:23] - INFO: ###  max_position_embeddings = 512
[2023-03-23 21:07:23] - INFO: ###  vocab_size = 21128
[2023-03-23 21:07:23] - INFO: ###  hidden_size = 768
[2023-03-23 21:07:23] - INFO: ###  num_hidden_layers = 12
[2023-03-23 21:07:23] - INFO: ###  num_attention_heads = 12
[2023-03-23 21:07:23] - INFO: ###  hidden_act = gelu
[2023-03-23 21:07:23] - INFO: ###  intermediate_size = 3072
[2023-03-23 21:07:23] - INFO: ###  pad_token_id = 0
[2023-03-23 21:07:23] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 21:07:23] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 21:07:23] - INFO: ###  type_vocab_size = 2
[2023-03-23 21:07:23] - INFO: ###  initializer_range = 0.02
[2023-03-23 21:07:23] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 21:07:23] - INFO: ###  directionality = bidi
[2023-03-23 21:07:23] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 21:07:23] - INFO: ###  model_type = bert
[2023-03-23 21:07:23] - INFO: ###  pooler_fc_size = 768
[2023-03-23 21:07:23] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 21:07:23] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 21:07:23] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 21:07:23] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 21:07:25] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 21:07:26] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 21:07:26] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/predict_None.pt 存在，直接载入缓存文件！
[2023-03-23 21:07:45] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 21:07:45] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 21:07:45] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 21:07:45] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 21:07:45] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 21:07:45] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 21:07:45] - INFO: ###  device = cuda:0
[2023-03-23 21:07:45] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 21:07:45] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 21:07:45] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 21:07:45] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 21:07:45] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 21:07:45] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 21:07:45] - INFO: ###  split_sep = _!_
[2023-03-23 21:07:45] - INFO: ###  is_sample_shuffle = True
[2023-03-23 21:07:45] - INFO: ###  batch_size = 32
[2023-03-23 21:07:45] - INFO: ###  max_sen_len = None
[2023-03-23 21:07:45] - INFO: ###  num_labels = 8191
[2023-03-23 21:07:45] - INFO: ###  epochs = 10
[2023-03-23 21:07:45] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 21:07:45] - INFO: ###  max_position_embeddings = 512
[2023-03-23 21:07:45] - INFO: ###  vocab_size = 21128
[2023-03-23 21:07:45] - INFO: ###  hidden_size = 768
[2023-03-23 21:07:45] - INFO: ###  num_hidden_layers = 12
[2023-03-23 21:07:45] - INFO: ###  num_attention_heads = 12
[2023-03-23 21:07:45] - INFO: ###  hidden_act = gelu
[2023-03-23 21:07:45] - INFO: ###  intermediate_size = 3072
[2023-03-23 21:07:45] - INFO: ###  pad_token_id = 0
[2023-03-23 21:07:45] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 21:07:45] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 21:07:45] - INFO: ###  type_vocab_size = 2
[2023-03-23 21:07:45] - INFO: ###  initializer_range = 0.02
[2023-03-23 21:07:45] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 21:07:45] - INFO: ###  directionality = bidi
[2023-03-23 21:07:45] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 21:07:45] - INFO: ###  model_type = bert
[2023-03-23 21:07:45] - INFO: ###  pooler_fc_size = 768
[2023-03-23 21:07:45] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 21:07:45] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 21:07:45] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 21:07:45] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 21:07:51] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 21:07:51] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 21:07:51] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 21:07:51] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 21:07:51] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 21:07:51] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 21:07:51] - INFO: ###  device = cuda:0
[2023-03-23 21:07:51] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 21:07:51] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 21:07:51] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 21:07:51] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 21:07:51] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 21:07:51] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 21:07:51] - INFO: ###  split_sep = _!_
[2023-03-23 21:07:51] - INFO: ###  is_sample_shuffle = True
[2023-03-23 21:07:51] - INFO: ###  batch_size = 32
[2023-03-23 21:07:51] - INFO: ###  max_sen_len = None
[2023-03-23 21:07:51] - INFO: ###  num_labels = 8191
[2023-03-23 21:07:51] - INFO: ###  epochs = 10
[2023-03-23 21:07:51] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 21:07:51] - INFO: ###  max_position_embeddings = 512
[2023-03-23 21:07:51] - INFO: ###  vocab_size = 21128
[2023-03-23 21:07:51] - INFO: ###  hidden_size = 768
[2023-03-23 21:07:51] - INFO: ###  num_hidden_layers = 12
[2023-03-23 21:07:51] - INFO: ###  num_attention_heads = 12
[2023-03-23 21:07:51] - INFO: ###  hidden_act = gelu
[2023-03-23 21:07:51] - INFO: ###  intermediate_size = 3072
[2023-03-23 21:07:51] - INFO: ###  pad_token_id = 0
[2023-03-23 21:07:51] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 21:07:51] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 21:07:51] - INFO: ###  type_vocab_size = 2
[2023-03-23 21:07:51] - INFO: ###  initializer_range = 0.02
[2023-03-23 21:07:51] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 21:07:51] - INFO: ###  directionality = bidi
[2023-03-23 21:07:51] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 21:07:51] - INFO: ###  model_type = bert
[2023-03-23 21:07:51] - INFO: ###  pooler_fc_size = 768
[2023-03-23 21:07:51] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 21:07:51] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 21:07:51] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 21:07:51] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 21:08:54] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 21:08:54] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 21:08:54] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 21:08:54] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 21:08:54] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 21:08:54] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 21:08:54] - INFO: ###  device = cuda:0
[2023-03-23 21:08:54] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 21:08:54] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 21:08:54] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 21:08:54] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 21:08:54] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 21:08:54] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 21:08:54] - INFO: ###  split_sep = _!_
[2023-03-23 21:08:54] - INFO: ###  is_sample_shuffle = True
[2023-03-23 21:08:54] - INFO: ###  batch_size = 32
[2023-03-23 21:08:54] - INFO: ###  max_sen_len = None
[2023-03-23 21:08:54] - INFO: ###  num_labels = 8191
[2023-03-23 21:08:54] - INFO: ###  epochs = 10
[2023-03-23 21:08:54] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 21:08:54] - INFO: ###  max_position_embeddings = 512
[2023-03-23 21:08:54] - INFO: ###  vocab_size = 21128
[2023-03-23 21:08:54] - INFO: ###  hidden_size = 768
[2023-03-23 21:08:54] - INFO: ###  num_hidden_layers = 12
[2023-03-23 21:08:54] - INFO: ###  num_attention_heads = 12
[2023-03-23 21:08:54] - INFO: ###  hidden_act = gelu
[2023-03-23 21:08:54] - INFO: ###  intermediate_size = 3072
[2023-03-23 21:08:54] - INFO: ###  pad_token_id = 0
[2023-03-23 21:08:54] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 21:08:54] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 21:08:54] - INFO: ###  type_vocab_size = 2
[2023-03-23 21:08:54] - INFO: ###  initializer_range = 0.02
[2023-03-23 21:08:54] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 21:08:54] - INFO: ###  directionality = bidi
[2023-03-23 21:08:54] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 21:08:54] - INFO: ###  model_type = bert
[2023-03-23 21:08:54] - INFO: ###  pooler_fc_size = 768
[2023-03-23 21:08:54] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 21:08:54] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 21:08:54] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 21:08:54] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 21:08:56] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 21:08:59] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 21:08:59] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/predict_None.pt 存在，直接载入缓存文件！
[2023-03-23 21:12:34] - INFO: 成功导入BERT配置文件 /root/project/BERT/bert_base_chinese/config.json
[2023-03-23 21:12:34] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 21:12:34] - INFO: ###  project_dir = /root/project/BERT
[2023-03-23 21:12:34] - INFO: ###  dataset_dir = /root/project/BERT/data/SingleSentenceClassification
[2023-03-23 21:12:34] - INFO: ###  pretrained_model_dir = /root/project/BERT/bert_base_chinese
[2023-03-23 21:12:34] - INFO: ###  vocab_path = /root/project/BERT/bert_base_chinese/vocab.txt
[2023-03-23 21:12:34] - INFO: ###  device = cuda:0
[2023-03-23 21:12:34] - INFO: ###  train_file_path = /root/project/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 21:12:34] - INFO: ###  val_file_path = /root/project/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 21:12:34] - INFO: ###  test_file_path = /root/project/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 21:12:34] - INFO: ###  predict_file_path = /root/project/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 21:12:34] - INFO: ###  model_save_dir = /root/project/BERT/cache
[2023-03-23 21:12:34] - INFO: ###  logs_save_dir = /root/project/BERT/logs
[2023-03-23 21:12:34] - INFO: ###  split_sep = _!_
[2023-03-23 21:12:34] - INFO: ###  is_sample_shuffle = True
[2023-03-23 21:12:34] - INFO: ###  batch_size = 32
[2023-03-23 21:12:34] - INFO: ###  max_sen_len = None
[2023-03-23 21:12:34] - INFO: ###  num_labels = 8191
[2023-03-23 21:12:34] - INFO: ###  epochs = 10
[2023-03-23 21:12:34] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 21:12:34] - INFO: ###  max_position_embeddings = 512
[2023-03-23 21:12:34] - INFO: ###  vocab_size = 21128
[2023-03-23 21:12:34] - INFO: ###  hidden_size = 768
[2023-03-23 21:12:34] - INFO: ###  num_hidden_layers = 12
[2023-03-23 21:12:34] - INFO: ###  num_attention_heads = 12
[2023-03-23 21:12:34] - INFO: ###  hidden_act = gelu
[2023-03-23 21:12:34] - INFO: ###  intermediate_size = 3072
[2023-03-23 21:12:34] - INFO: ###  pad_token_id = 0
[2023-03-23 21:12:34] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 21:12:34] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 21:12:34] - INFO: ###  type_vocab_size = 2
[2023-03-23 21:12:34] - INFO: ###  initializer_range = 0.02
[2023-03-23 21:12:34] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 21:12:34] - INFO: ###  directionality = bidi
[2023-03-23 21:12:34] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 21:12:34] - INFO: ###  model_type = bert
[2023-03-23 21:12:34] - INFO: ###  pooler_fc_size = 768
[2023-03-23 21:12:34] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 21:12:34] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 21:12:34] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 21:12:34] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 21:12:37] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 21:12:39] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 21:12:39] - INFO: 缓存文件 /root/project/BERT/data/SingleSentenceClassification/predict_None.pt 不存在，重新处理并缓存！
[2023-03-23 21:24:44] - INFO: 成功导入BERT配置文件 /root/project/final_model/BERT/bert_base_chinese/config.json
[2023-03-23 21:24:44] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 21:24:44] - INFO: ###  project_dir = /root/project/final_model/BERT
[2023-03-23 21:24:44] - INFO: ###  dataset_dir = /root/project/final_model/BERT/data/SingleSentenceClassification
[2023-03-23 21:24:44] - INFO: ###  pretrained_model_dir = /root/project/final_model/BERT/bert_base_chinese
[2023-03-23 21:24:44] - INFO: ###  vocab_path = /root/project/final_model/BERT/bert_base_chinese/vocab.txt
[2023-03-23 21:24:44] - INFO: ###  device = cuda:0
[2023-03-23 21:24:44] - INFO: ###  train_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 21:24:44] - INFO: ###  val_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 21:24:44] - INFO: ###  test_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 21:24:44] - INFO: ###  predict_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 21:24:44] - INFO: ###  model_save_dir = /root/project/final_model/BERT/cache
[2023-03-23 21:24:44] - INFO: ###  logs_save_dir = /root/project/final_model/BERT/logs
[2023-03-23 21:24:44] - INFO: ###  split_sep = _!_
[2023-03-23 21:24:44] - INFO: ###  is_sample_shuffle = True
[2023-03-23 21:24:44] - INFO: ###  batch_size = 32
[2023-03-23 21:24:44] - INFO: ###  max_sen_len = None
[2023-03-23 21:24:44] - INFO: ###  num_labels = 8191
[2023-03-23 21:24:44] - INFO: ###  epochs = 10
[2023-03-23 21:24:44] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 21:24:44] - INFO: ###  max_position_embeddings = 512
[2023-03-23 21:24:44] - INFO: ###  vocab_size = 21128
[2023-03-23 21:24:44] - INFO: ###  hidden_size = 768
[2023-03-23 21:24:44] - INFO: ###  num_hidden_layers = 12
[2023-03-23 21:24:44] - INFO: ###  num_attention_heads = 12
[2023-03-23 21:24:44] - INFO: ###  hidden_act = gelu
[2023-03-23 21:24:44] - INFO: ###  intermediate_size = 3072
[2023-03-23 21:24:44] - INFO: ###  pad_token_id = 0
[2023-03-23 21:24:44] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 21:24:44] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 21:24:44] - INFO: ###  type_vocab_size = 2
[2023-03-23 21:24:44] - INFO: ###  initializer_range = 0.02
[2023-03-23 21:24:44] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 21:24:44] - INFO: ###  directionality = bidi
[2023-03-23 21:24:44] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 21:24:44] - INFO: ###  model_type = bert
[2023-03-23 21:24:44] - INFO: ###  pooler_fc_size = 768
[2023-03-23 21:24:44] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 21:24:44] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 21:24:44] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 21:24:44] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 21:24:46] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 21:24:48] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 21:24:49] - INFO: 缓存文件 /root/project/final_model/BERT/data/SingleSentenceClassification/predict_None.pt 不存在，重新处理并缓存！
[2023-03-23 21:26:22] - INFO: 成功导入BERT配置文件 /root/project/final_model/BERT/bert_base_chinese/config.json
[2023-03-23 21:26:22] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 21:26:22] - INFO: ###  project_dir = /root/project/final_model/BERT
[2023-03-23 21:26:22] - INFO: ###  dataset_dir = /root/project/final_model/BERT/data/SingleSentenceClassification
[2023-03-23 21:26:22] - INFO: ###  pretrained_model_dir = /root/project/final_model/BERT/bert_base_chinese
[2023-03-23 21:26:22] - INFO: ###  vocab_path = /root/project/final_model/BERT/bert_base_chinese/vocab.txt
[2023-03-23 21:26:22] - INFO: ###  device = cuda:0
[2023-03-23 21:26:22] - INFO: ###  train_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 21:26:22] - INFO: ###  val_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 21:26:22] - INFO: ###  test_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 21:26:22] - INFO: ###  predict_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 21:26:22] - INFO: ###  model_save_dir = /root/project/final_model/BERT/cache
[2023-03-23 21:26:22] - INFO: ###  logs_save_dir = /root/project/final_model/BERT/logs
[2023-03-23 21:26:22] - INFO: ###  split_sep = _!_
[2023-03-23 21:26:22] - INFO: ###  is_sample_shuffle = True
[2023-03-23 21:26:22] - INFO: ###  batch_size = 32
[2023-03-23 21:26:22] - INFO: ###  max_sen_len = None
[2023-03-23 21:26:22] - INFO: ###  num_labels = 8191
[2023-03-23 21:26:22] - INFO: ###  epochs = 10
[2023-03-23 21:26:22] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 21:26:22] - INFO: ###  max_position_embeddings = 512
[2023-03-23 21:26:22] - INFO: ###  vocab_size = 21128
[2023-03-23 21:26:22] - INFO: ###  hidden_size = 768
[2023-03-23 21:26:22] - INFO: ###  num_hidden_layers = 12
[2023-03-23 21:26:22] - INFO: ###  num_attention_heads = 12
[2023-03-23 21:26:22] - INFO: ###  hidden_act = gelu
[2023-03-23 21:26:22] - INFO: ###  intermediate_size = 3072
[2023-03-23 21:26:22] - INFO: ###  pad_token_id = 0
[2023-03-23 21:26:22] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 21:26:22] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 21:26:22] - INFO: ###  type_vocab_size = 2
[2023-03-23 21:26:22] - INFO: ###  initializer_range = 0.02
[2023-03-23 21:26:22] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 21:26:22] - INFO: ###  directionality = bidi
[2023-03-23 21:26:22] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 21:26:22] - INFO: ###  model_type = bert
[2023-03-23 21:26:22] - INFO: ###  pooler_fc_size = 768
[2023-03-23 21:26:22] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 21:26:22] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 21:26:22] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 21:26:22] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 21:26:24] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 21:26:27] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 21:26:27] - INFO: 缓存文件 /root/project/final_model/BERT/data/SingleSentenceClassification/predict_None.pt 不存在，重新处理并缓存！
[2023-03-23 21:31:01] - INFO: 成功导入BERT配置文件 /root/project/final_model/BERT/bert_base_chinese/config.json
[2023-03-23 21:31:01] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 21:31:01] - INFO: ###  project_dir = /root/project/final_model/BERT
[2023-03-23 21:31:01] - INFO: ###  dataset_dir = /root/project/final_model/BERT/data/SingleSentenceClassification
[2023-03-23 21:31:01] - INFO: ###  pretrained_model_dir = /root/project/final_model/BERT/bert_base_chinese
[2023-03-23 21:31:01] - INFO: ###  vocab_path = /root/project/final_model/BERT/bert_base_chinese/vocab.txt
[2023-03-23 21:31:01] - INFO: ###  device = cuda:0
[2023-03-23 21:31:01] - INFO: ###  train_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 21:31:01] - INFO: ###  val_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 21:31:01] - INFO: ###  test_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 21:31:01] - INFO: ###  predict_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 21:31:01] - INFO: ###  model_save_dir = /root/project/final_model/BERT/cache
[2023-03-23 21:31:01] - INFO: ###  logs_save_dir = /root/project/final_model/BERT/logs
[2023-03-23 21:31:01] - INFO: ###  split_sep = _!_
[2023-03-23 21:31:01] - INFO: ###  is_sample_shuffle = True
[2023-03-23 21:31:01] - INFO: ###  batch_size = 32
[2023-03-23 21:31:01] - INFO: ###  max_sen_len = None
[2023-03-23 21:31:01] - INFO: ###  num_labels = 8191
[2023-03-23 21:31:01] - INFO: ###  epochs = 10
[2023-03-23 21:31:01] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 21:31:01] - INFO: ###  max_position_embeddings = 512
[2023-03-23 21:31:01] - INFO: ###  vocab_size = 21128
[2023-03-23 21:31:01] - INFO: ###  hidden_size = 768
[2023-03-23 21:31:01] - INFO: ###  num_hidden_layers = 12
[2023-03-23 21:31:01] - INFO: ###  num_attention_heads = 12
[2023-03-23 21:31:01] - INFO: ###  hidden_act = gelu
[2023-03-23 21:31:01] - INFO: ###  intermediate_size = 3072
[2023-03-23 21:31:01] - INFO: ###  pad_token_id = 0
[2023-03-23 21:31:01] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 21:31:01] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 21:31:01] - INFO: ###  type_vocab_size = 2
[2023-03-23 21:31:01] - INFO: ###  initializer_range = 0.02
[2023-03-23 21:31:01] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 21:31:01] - INFO: ###  directionality = bidi
[2023-03-23 21:31:01] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 21:31:01] - INFO: ###  model_type = bert
[2023-03-23 21:31:01] - INFO: ###  pooler_fc_size = 768
[2023-03-23 21:31:01] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 21:31:01] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 21:31:01] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 21:31:01] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 21:31:02] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 21:31:05] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 21:31:05] - INFO: 缓存文件 /root/project/final_model/BERT/data/SingleSentenceClassification/predict_None.pt 不存在，重新处理并缓存！
[2023-03-23 21:41:32] - INFO: 成功导入BERT配置文件 /root/project/final_model/BERT/bert_base_chinese/config.json
[2023-03-23 21:41:32] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 21:41:32] - INFO: ###  project_dir = /root/project/final_model/BERT
[2023-03-23 21:41:32] - INFO: ###  dataset_dir = /root/project/final_model/BERT/data/SingleSentenceClassification
[2023-03-23 21:41:32] - INFO: ###  pretrained_model_dir = /root/project/final_model/BERT/bert_base_chinese
[2023-03-23 21:41:32] - INFO: ###  vocab_path = /root/project/final_model/BERT/bert_base_chinese/vocab.txt
[2023-03-23 21:41:32] - INFO: ###  device = cuda:0
[2023-03-23 21:41:32] - INFO: ###  train_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 21:41:32] - INFO: ###  val_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 21:41:32] - INFO: ###  test_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 21:41:32] - INFO: ###  predict_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 21:41:32] - INFO: ###  model_save_dir = /root/project/final_model/BERT/cache
[2023-03-23 21:41:32] - INFO: ###  logs_save_dir = /root/project/final_model/BERT/logs
[2023-03-23 21:41:32] - INFO: ###  split_sep = _!_
[2023-03-23 21:41:32] - INFO: ###  is_sample_shuffle = True
[2023-03-23 21:41:32] - INFO: ###  batch_size = 32
[2023-03-23 21:41:32] - INFO: ###  max_sen_len = None
[2023-03-23 21:41:32] - INFO: ###  num_labels = 8191
[2023-03-23 21:41:32] - INFO: ###  epochs = 10
[2023-03-23 21:41:32] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 21:41:32] - INFO: ###  max_position_embeddings = 512
[2023-03-23 21:41:32] - INFO: ###  vocab_size = 21128
[2023-03-23 21:41:32] - INFO: ###  hidden_size = 768
[2023-03-23 21:41:32] - INFO: ###  num_hidden_layers = 12
[2023-03-23 21:41:32] - INFO: ###  num_attention_heads = 12
[2023-03-23 21:41:32] - INFO: ###  hidden_act = gelu
[2023-03-23 21:41:32] - INFO: ###  intermediate_size = 3072
[2023-03-23 21:41:32] - INFO: ###  pad_token_id = 0
[2023-03-23 21:41:32] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 21:41:32] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 21:41:32] - INFO: ###  type_vocab_size = 2
[2023-03-23 21:41:32] - INFO: ###  initializer_range = 0.02
[2023-03-23 21:41:32] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 21:41:32] - INFO: ###  directionality = bidi
[2023-03-23 21:41:32] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 21:41:32] - INFO: ###  model_type = bert
[2023-03-23 21:41:32] - INFO: ###  pooler_fc_size = 768
[2023-03-23 21:41:32] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 21:41:32] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 21:41:32] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 21:41:32] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 21:41:34] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 21:41:36] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 21:41:36] - INFO: 缓存文件 /root/project/final_model/BERT/data/SingleSentenceClassification/predict_None.pt 不存在，重新处理并缓存！
[2023-03-23 21:51:21] - INFO: 成功导入BERT配置文件 /root/project/final_model/BERT/bert_base_chinese/config.json
[2023-03-23 21:51:21] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 21:51:21] - INFO: ###  project_dir = /root/project/final_model/BERT
[2023-03-23 21:51:21] - INFO: ###  dataset_dir = /root/project/final_model/BERT/data/SingleSentenceClassification
[2023-03-23 21:51:21] - INFO: ###  pretrained_model_dir = /root/project/final_model/BERT/bert_base_chinese
[2023-03-23 21:51:21] - INFO: ###  vocab_path = /root/project/final_model/BERT/bert_base_chinese/vocab.txt
[2023-03-23 21:51:21] - INFO: ###  device = cuda:0
[2023-03-23 21:51:21] - INFO: ###  train_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 21:51:21] - INFO: ###  val_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 21:51:21] - INFO: ###  test_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 21:51:21] - INFO: ###  predict_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 21:51:21] - INFO: ###  model_save_dir = /root/project/final_model/BERT/cache
[2023-03-23 21:51:21] - INFO: ###  logs_save_dir = /root/project/final_model/BERT/logs
[2023-03-23 21:51:21] - INFO: ###  split_sep = _!_
[2023-03-23 21:51:21] - INFO: ###  is_sample_shuffle = True
[2023-03-23 21:51:21] - INFO: ###  batch_size = 32
[2023-03-23 21:51:21] - INFO: ###  max_sen_len = None
[2023-03-23 21:51:21] - INFO: ###  num_labels = 8191
[2023-03-23 21:51:21] - INFO: ###  epochs = 10
[2023-03-23 21:51:21] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 21:51:21] - INFO: ###  max_position_embeddings = 512
[2023-03-23 21:51:21] - INFO: ###  vocab_size = 21128
[2023-03-23 21:51:21] - INFO: ###  hidden_size = 768
[2023-03-23 21:51:21] - INFO: ###  num_hidden_layers = 12
[2023-03-23 21:51:21] - INFO: ###  num_attention_heads = 12
[2023-03-23 21:51:21] - INFO: ###  hidden_act = gelu
[2023-03-23 21:51:21] - INFO: ###  intermediate_size = 3072
[2023-03-23 21:51:21] - INFO: ###  pad_token_id = 0
[2023-03-23 21:51:21] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 21:51:21] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 21:51:21] - INFO: ###  type_vocab_size = 2
[2023-03-23 21:51:21] - INFO: ###  initializer_range = 0.02
[2023-03-23 21:51:21] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 21:51:21] - INFO: ###  directionality = bidi
[2023-03-23 21:51:21] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 21:51:21] - INFO: ###  model_type = bert
[2023-03-23 21:51:21] - INFO: ###  pooler_fc_size = 768
[2023-03-23 21:51:21] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 21:51:21] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 21:51:21] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 21:51:21] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 21:51:23] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 21:51:25] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 21:51:25] - INFO: 缓存文件 /root/project/final_model/BERT/data/SingleSentenceClassification/predict_None.pt 不存在，重新处理并缓存！
[2023-03-23 21:51:46] - INFO: 成功导入BERT配置文件 /root/project/final_model/BERT/bert_base_chinese/config.json
[2023-03-23 21:51:46] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 21:51:46] - INFO: ###  project_dir = /root/project/final_model/BERT
[2023-03-23 21:51:46] - INFO: ###  dataset_dir = /root/project/final_model/BERT/data/SingleSentenceClassification
[2023-03-23 21:51:46] - INFO: ###  pretrained_model_dir = /root/project/final_model/BERT/bert_base_chinese
[2023-03-23 21:51:46] - INFO: ###  vocab_path = /root/project/final_model/BERT/bert_base_chinese/vocab.txt
[2023-03-23 21:51:46] - INFO: ###  device = cuda:0
[2023-03-23 21:51:46] - INFO: ###  train_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 21:51:46] - INFO: ###  val_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 21:51:46] - INFO: ###  test_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 21:51:46] - INFO: ###  predict_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 21:51:46] - INFO: ###  model_save_dir = /root/project/final_model/BERT/cache
[2023-03-23 21:51:46] - INFO: ###  logs_save_dir = /root/project/final_model/BERT/logs
[2023-03-23 21:51:46] - INFO: ###  split_sep = _!_
[2023-03-23 21:51:46] - INFO: ###  is_sample_shuffle = True
[2023-03-23 21:51:46] - INFO: ###  batch_size = 32
[2023-03-23 21:51:46] - INFO: ###  max_sen_len = None
[2023-03-23 21:51:46] - INFO: ###  num_labels = 8191
[2023-03-23 21:51:46] - INFO: ###  epochs = 10
[2023-03-23 21:51:46] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 21:51:46] - INFO: ###  max_position_embeddings = 512
[2023-03-23 21:51:46] - INFO: ###  vocab_size = 21128
[2023-03-23 21:51:46] - INFO: ###  hidden_size = 768
[2023-03-23 21:51:46] - INFO: ###  num_hidden_layers = 12
[2023-03-23 21:51:46] - INFO: ###  num_attention_heads = 12
[2023-03-23 21:51:46] - INFO: ###  hidden_act = gelu
[2023-03-23 21:51:46] - INFO: ###  intermediate_size = 3072
[2023-03-23 21:51:46] - INFO: ###  pad_token_id = 0
[2023-03-23 21:51:46] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 21:51:46] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 21:51:46] - INFO: ###  type_vocab_size = 2
[2023-03-23 21:51:46] - INFO: ###  initializer_range = 0.02
[2023-03-23 21:51:46] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 21:51:46] - INFO: ###  directionality = bidi
[2023-03-23 21:51:46] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 21:51:46] - INFO: ###  model_type = bert
[2023-03-23 21:51:46] - INFO: ###  pooler_fc_size = 768
[2023-03-23 21:51:46] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 21:51:46] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 21:51:46] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 21:51:46] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 21:51:48] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 21:51:48] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 21:51:48] - INFO: 缓存文件 /root/project/final_model/BERT/data/SingleSentenceClassification/predict_None.pt 不存在，重新处理并缓存！
[2023-03-23 21:58:05] - INFO: 成功导入BERT配置文件 /root/project/final_model/BERT/bert_base_chinese/config.json
[2023-03-23 21:58:05] - INFO:  ### 将当前配置打印到日志文件中 
[2023-03-23 21:58:05] - INFO: ###  project_dir = /root/project/final_model/BERT
[2023-03-23 21:58:05] - INFO: ###  dataset_dir = /root/project/final_model/BERT/data/SingleSentenceClassification
[2023-03-23 21:58:05] - INFO: ###  pretrained_model_dir = /root/project/final_model/BERT/bert_base_chinese
[2023-03-23 21:58:05] - INFO: ###  vocab_path = /root/project/final_model/BERT/bert_base_chinese/vocab.txt
[2023-03-23 21:58:05] - INFO: ###  device = cuda:0
[2023-03-23 21:58:05] - INFO: ###  train_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/train.txt
[2023-03-23 21:58:05] - INFO: ###  val_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/val.txt
[2023-03-23 21:58:05] - INFO: ###  test_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/test.txt
[2023-03-23 21:58:05] - INFO: ###  predict_file_path = /root/project/final_model/BERT/data/SingleSentenceClassification/predict.txt
[2023-03-23 21:58:05] - INFO: ###  model_save_dir = /root/project/final_model/BERT/cache
[2023-03-23 21:58:05] - INFO: ###  logs_save_dir = /root/project/final_model/BERT/logs
[2023-03-23 21:58:05] - INFO: ###  split_sep = _!_
[2023-03-23 21:58:05] - INFO: ###  is_sample_shuffle = True
[2023-03-23 21:58:05] - INFO: ###  batch_size = 32
[2023-03-23 21:58:05] - INFO: ###  max_sen_len = None
[2023-03-23 21:58:05] - INFO: ###  num_labels = 8191
[2023-03-23 21:58:05] - INFO: ###  epochs = 10
[2023-03-23 21:58:05] - INFO: ###  model_val_per_epoch = 2
[2023-03-23 21:58:05] - INFO: ###  max_position_embeddings = 512
[2023-03-23 21:58:05] - INFO: ###  vocab_size = 21128
[2023-03-23 21:58:05] - INFO: ###  hidden_size = 768
[2023-03-23 21:58:05] - INFO: ###  num_hidden_layers = 12
[2023-03-23 21:58:05] - INFO: ###  num_attention_heads = 12
[2023-03-23 21:58:05] - INFO: ###  hidden_act = gelu
[2023-03-23 21:58:05] - INFO: ###  intermediate_size = 3072
[2023-03-23 21:58:05] - INFO: ###  pad_token_id = 0
[2023-03-23 21:58:05] - INFO: ###  hidden_dropout_prob = 0.1
[2023-03-23 21:58:05] - INFO: ###  attention_probs_dropout_prob = 0.1
[2023-03-23 21:58:05] - INFO: ###  type_vocab_size = 2
[2023-03-23 21:58:05] - INFO: ###  initializer_range = 0.02
[2023-03-23 21:58:05] - INFO: ###  architectures = ['BertForMaskedLM']
[2023-03-23 21:58:05] - INFO: ###  directionality = bidi
[2023-03-23 21:58:05] - INFO: ###  layer_norm_eps = 1e-12
[2023-03-23 21:58:05] - INFO: ###  model_type = bert
[2023-03-23 21:58:05] - INFO: ###  pooler_fc_size = 768
[2023-03-23 21:58:05] - INFO: ###  pooler_num_attention_heads = 12
[2023-03-23 21:58:05] - INFO: ###  pooler_num_fc_layers = 3
[2023-03-23 21:58:05] - INFO: ###  pooler_size_per_head = 128
[2023-03-23 21:58:05] - INFO: ###  pooler_type = first_token_transform
[2023-03-23 21:58:06] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现
[2023-03-23 21:58:07] - INFO: ## 成功载入已有模型，进行预测......
[2023-03-23 21:58:07] - INFO: 缓存文件 /root/project/final_model/BERT/data/SingleSentenceClassification/predict_None.pt 不存在，重新处理并缓存！
